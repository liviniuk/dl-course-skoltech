{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Deep Learning (7 points)\n",
    "\n",
    "Today we're gonna apply the newly learned DL tools for sequence processing to the task of predicting job salary.\n",
    "\n",
    "Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the assignment core (orignally written for theano/tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the challenge\n",
    "For starters, let's download the data from __[here](https://yadi.sk/d/vVEOWPFY3NruT7)__.\n",
    "\n",
    "You can also get it from the competition [page](https://www.kaggle.com/c/job-salary-prediction/data) (in that case, pick `Train_rev1.*`).\n",
    "\n",
    "\n",
    "Our task is to predict one number, __SalaryNormalized__, in the sense of minimizing __Mean Absolute Error__.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>\n",
    "\n",
    "To do so, our model ca access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__.\n",
    "\n",
    "\n",
    "You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225432    Just Social Care has a fantastic opportunity f...\n",
       "173921    Our client, a medical devices company, is look...\n",
       "187378    A major energy and environmental consultancy i...\n",
       "Name: FullDescription, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast nan to string\n",
    "\n",
    "data.sample(3)['FullDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164996</th>\n",
       "      <td>71287994</td>\n",
       "      <td>Primary Teachers in Hall Green and Shirley needed</td>\n",
       "      <td>Aspire People are an educational recruitment a...</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aspire People</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>105 - 145 per day</td>\n",
       "      <td>30000</td>\n",
       "      <td>MyUkJobs</td>\n",
       "      <td>10.308986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22446</th>\n",
       "      <td>67393476</td>\n",
       "      <td>Search and Display Project Manager</td>\n",
       "      <td>A high profile brand is seeking an enthusiasti...</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>full_time</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Harvey Nash</td>\n",
       "      <td>PR, Advertising &amp; Marketing Jobs</td>\n",
       "      <td>45000 - 55000 per annum</td>\n",
       "      <td>50000</td>\n",
       "      <td>jobs.newstatesman.com</td>\n",
       "      <td>10.819798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32622</th>\n",
       "      <td>68098820</td>\n",
       "      <td>Business Analyst  Oxford, Oxfordshire</td>\n",
       "      <td>Business Analyst  Oxford, Oxfordshire. My tech...</td>\n",
       "      <td>Oxford Oxfordshire South East</td>\n",
       "      <td>Oxford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Evolution Recruitment Solutions Ltd</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>45,000 - 55,000</td>\n",
       "      <td>50000</td>\n",
       "      <td>totaljobs.com</td>\n",
       "      <td>10.819798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "164996  71287994  Primary Teachers in Hall Green and Shirley needed   \n",
       "22446   67393476                 Search and Display Project Manager   \n",
       "32622   68098820              Business Analyst  Oxford, Oxfordshire   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "164996  Aspire People are an educational recruitment a...   \n",
       "22446   A high profile brand is seeking an enthusiasti...   \n",
       "32622   Business Analyst  Oxford, Oxfordshire. My tech...   \n",
       "\n",
       "                          LocationRaw LocationNormalized ContractType  \\\n",
       "164996                     Birmingham         Birmingham          NaN   \n",
       "22446                          London             London    full_time   \n",
       "32622   Oxford Oxfordshire South East             Oxford          NaN   \n",
       "\n",
       "       ContractTime                              Company  \\\n",
       "164996          NaN                        Aspire People   \n",
       "22446     permanent                          Harvey Nash   \n",
       "32622     permanent  Evolution Recruitment Solutions Ltd   \n",
       "\n",
       "                                Category                SalaryRaw  \\\n",
       "164996                     Teaching Jobs        105 - 145 per day   \n",
       "22446   PR, Advertising & Marketing Jobs  45000 - 55000 per annum   \n",
       "32622                            IT Jobs          45,000 - 55,000   \n",
       "\n",
       "        SalaryNormalized             SourceName  Log1pSalary  \n",
       "164996             30000               MyUkJobs    10.308986  \n",
       "22446              50000  jobs.newstatesman.com    10.819798  \n",
       "32622              50000          totaljobs.com    10.819798  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP part\n",
    "\n",
    "To even begin training our neural network, we're gonna need to preprocess the text features: tokenize it and build the token vocabularies.\n",
    "\n",
    "Since it is not an NLP course, we're gonna use simple built-in NLTK tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "0         Engineering Systems Analyst\n",
      "100000                   HR Assistant\n",
      "200000           Senior EC&I Engineer\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\")\n",
    "print(data[\"Title\"][::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(lambda l: ' '.join(tokenizer.tokenize(str(l).lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assume that our text is a space-separated list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "0         engineering systems analyst\n",
      "100000                   hr assistant\n",
      "200000         senior ec & i engineer\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"After\")\n",
    "print(data[\"Title\"][::100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's see how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter()\n",
    "\n",
    "# Count how many times does each token occur in \"Title\" and \"FullDescription\"\n",
    "# <YOUR CODE HERE>\n",
    "for col in text_columns:\n",
    "    data[col].apply(lambda l: token_counts.update(l.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 202704\n",
      "('and', 2657388)\n",
      "('.', 2523216)\n",
      "(',', 2318606)\n",
      "('the', 2080994)\n",
      "('to', 2019884)\n",
      "...\n",
      "('stephanietraveltraderecruitmnt', 1)\n",
      "('ruabon', 1)\n",
      "('lowehays', 1)\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Counts')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many words are there for each count\n",
    "\n",
    "_ = plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.1__ Get a list of all tokens that occur at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "# <YOUR CODE HERE>\n",
    "tokens = []\n",
    "for token in token_counts.most_common():\n",
    "    if token[1] >= min_count:\n",
    "        tokens.append(token[0]) \n",
    "\n",
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens left: 34158\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens left:\", len(tokens))\n",
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,35000)\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__ Build an inverse token index: a dictionary from token(string) to it's index in `tokens` (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {t: idx for idx,t in enumerate(tokens)} # <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into torch-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineering systems analyst\n",
      "hr assistant\n",
      "senior ec & i engineer\n",
      "\n",
      "Matrix:\n",
      "[[ 136  114  269    1    1]\n",
      " [ 311  236    1    1    1]\n",
      " [ 100 4120  135  388   83]]\n"
     ]
    }
   ],
   "source": [
    "#### print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement tf-idf, target averaging or pseudo-counter-based encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, separator='=', sort=True,\n",
       "        sparse=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data science part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  220291\n",
      "Validation size =  24477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=True, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a pytorch-friendly dict from the batch data.\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        data = data.sample(batch_size, replace=replace)\n",
    "    \n",
    "    batch = {}\n",
    "    for col in text_columns:\n",
    "        batch[col] = as_matrix(data[col].values, max_len)\n",
    "    \n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': array([[295,   8, 559,   1],\n",
       "        [469, 283,  36,  34],\n",
       "        [133, 863,   1,   1]], dtype=int32),\n",
       " 'FullDescription': array([[  252,     6,   247, 10079,     4,     7,   102,    92,   654,\n",
       "           242],\n",
       "        [ 6927,     4,   469,   283,    36,    34,     4,    35,   163,\n",
       "           525],\n",
       "        [   12,    14,    85,     7,   449,  1541,   159,    73,   181,\n",
       "           214]], dtype=int32),\n",
       " 'Categorical': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'Log1pSalary': array([10.821936, 10.571342, 10.043293], dtype=float32)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(data_train, 3, max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's talk deep learning\n",
    "\n",
    "Out model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/Practical_DL/raw/master/homework04/conv_salary_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, both text vectorizers shall use 1d convolutions, followed by global pooling over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-trained word2vec from http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "import gensim\n",
    "\n",
    "pretrained_enc = True\n",
    "\n",
    "if pretrained_enc == True:\n",
    "    word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        './model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    mean_std = word2vec.vectors.std(axis=0).mean()\n",
    "\n",
    "    google_emb = nn.Embedding(len(tokens), 300, padding_idx=PAD_IX)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if not token in word2vec:\n",
    "            # Try uppercase\n",
    "            token = token[0].upper() + token[1:]\n",
    "        if not token in word2vec:\n",
    "            # Try CAPS\n",
    "            token = token[0] + token[1:].upper()\n",
    "        if token in word2vec:\n",
    "            google_emb.weight.data[i] = torch.tensor(word2vec[token], dtype=torch.float32)\n",
    "        else:\n",
    "            # Because Google's word2vec has different distribution\n",
    "            google_emb.weight.data[i] *= mean_std\n",
    "            \n",
    "    del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, pretrained_enc=pretrained_enc):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if pretrained_enc == True:\n",
    "            self.emb = google_emb\n",
    "        else:\n",
    "            self.emb = nn.Embedding(n_tokens, 300, padding_idx=PAD_IX)\n",
    "\n",
    "        # Parallel Conv\n",
    "        self.conv1 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv_layers = [self.conv1, self.conv2, self.conv3, self.conv3, self.conv4, self.conv5]\n",
    "        \n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        # <YOUR CODE>\n",
    "        h = torch.cat([conv(h) for conv in self.conv_layers], dim=2)\n",
    "        h = self.pool1(h)\n",
    "        h = self.dense(h.relu())\n",
    "        \n",
    "        return h # <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = TitleEncoder(out_size=64).cuda()\n",
    "\n",
    "dummy_x = Variable(torch.LongTensor(generate_batch(data_train, 3)['Title']))\n",
    "dummy_v = title_encoder(dummy_x.cuda())\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ Create description encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoder for job descriptions.\n",
    "# Use any means you want so long as it's torch.nn.Module.\n",
    "# <YOUR CODE HERE>\n",
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, pretrained_enc=pretrained_enc):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for descriptions.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if pretrained_enc == True:\n",
    "            self.emb = google_emb\n",
    "        else:\n",
    "            self.emb = nn.Embedding(n_tokens, 300, padding_idx=PAD_IX)\n",
    "\n",
    "        # Parallel Conv\n",
    "        self.conv1 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(300, out_size, kernel_size=3, padding=1)\n",
    "        self.conv_layers = [self.conv1, self.conv2, self.conv3, self.conv3, self.conv4, self.conv5]\n",
    "        \n",
    "        self.pool1 = GlobalMaxPooling()        \n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "\n",
    "        h = torch.cat([conv(h) for conv in self.conv_layers], dim=2)\n",
    "        h = self.pool1(h)\n",
    "        h = self.dense(h.relu())\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine too\n"
     ]
    }
   ],
   "source": [
    "# <Create description encoder>\n",
    "desc_encoder = DescriptionEncoder(out_size=64).cuda() \n",
    "\n",
    "dummy_x = Variable(torch.LongTensor(generate_batch(data_train, 3)['FullDescription']))\n",
    "dummy_v = desc_encoder(dummy_x.cuda())\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "del desc_encoder\n",
    "print(\"Seems fine too\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2__ Build one network ~~to rule them all~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoder(out_size=64)\n",
    "        self.desc_encoder = DescriptionEncoder(out_size=64) # <YOUR CODE>\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        # <YOUR CODE>\n",
    "        self.cat_encoder = nn.Sequential(nn.Linear(n_cat_features, 64*5),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.BatchNorm1d(64*5), # improves MAE 2500 -> 2386\n",
    "                                         nn.Linear(64*5, 64)\n",
    "                                        )\n",
    "        \n",
    "        \n",
    "#         self.cat_encoder = nn.Sequential(nn.Linear(n_cat_features, 64*5),\n",
    "#                                          nn.ReLU(),\n",
    "#                                          nn.BatchNorm1d(64*5), # improves MAE 2500 -> 2386\n",
    "#                                          nn.Linear(64*5, 64*3),\n",
    "#                                          nn.ReLU(),\n",
    "#                                          nn.BatchNorm1d(64*3),\n",
    "#                                          nn.Linear(64*3, 64),\n",
    "#                                         )\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        # <YOUR CODE>\n",
    "        self.output = nn.Sequential(nn.ReLU(), # obviously, improves the results. MAE 2500 -> 2153\n",
    "                                    nn.Linear(64*3, 64*5),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(64*5, 1)\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "#         self.output = nn.Sequential(nn.ReLU(), # obviously, improves the results. MAE 2500 -> 2153\n",
    "#                                     nn.Linear(64*3, 64*5),\n",
    "#                                     nn.ReLU(),\n",
    "#                                     nn.Linear(64*5, 64*3),\n",
    "#                                     nn.ReLU(),\n",
    "#                                     nn.Linear(64*3, 1)\n",
    "#                                    )\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix) # <YOUR CODE>\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.cat_encoder(cat_features) # <YOUR CODE>\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "        out = self.output(joint_h)[:,0] # <YOUR CODE>\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return out # <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# Save embeddings for other experiments\n",
    "# original_google_emb_state_dict = deepcopy(google_emb.state_dict())\n",
    "\n",
    "# Create model and Optimizaer\n",
    "model = FullNetwork().cuda()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Load original embeddings\n",
    "google_emb.load_state_dict(original_google_emb_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it on one batch\n",
    "\n",
    "batch = generate_batch(data_train, 32)\n",
    "\n",
    "title_ix = torch.tensor(batch[\"Title\"], dtype=torch.int64).cuda()\n",
    "desc_ix = torch.tensor(batch[\"FullDescription\"], dtype=torch.int64).cuda()\n",
    "cat_features = torch.tensor(batch[\"Categorical\"], dtype=torch.float32).cuda()\n",
    "reference = torch.tensor(batch[target_column], dtype=torch.float32).cuda()\n",
    "\n",
    "prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "assert len(prediction.shape) == 1 and prediction.shape[0] == title_ix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(reference, prediction):\n",
    "    \"\"\"\n",
    "    Computes objective for minimization.\n",
    "    By deafult we minimize MSE, but you are encouraged to try mix up MSE, MAE, huber loss, etc.\n",
    "    \"\"\"\n",
    "    return torch.mean((prediction - reference) ** 2)\n",
    "\n",
    "def compute_mae(reference, prediction):\n",
    "    \"\"\" Compute MAE on actual salary, assuming your model outputs log1p(salary)\"\"\"\n",
    "    return torch.abs(torch.exp(reference - 1) - torch.exp(prediction - 1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(reference, prediction)\n",
    "dummy_grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "for grad in dummy_grads:\n",
    "    assert grad is not None and not (grad == 0).all(), \"Some model parameters received zero grads. \" \\\n",
    "                                                       \"Double-check that your model uses all it's layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch(data.iloc[indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "max_len = 100\n",
    "batch_size = 100 # 32\n",
    "batches_per_epoch = 200 #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "# \"Optimizing seriously\" params\n",
    "best_val_mae = None\n",
    "patience = 5\n",
    "\n",
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to freeze/unfreeze embedding weights for training.\n",
    "# google_emb.weight.requires_grad = False\n",
    "google_emb.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAE1CAYAAAD6akEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xl8VeW1//HPysnJSCAJJBISEFCUScaAtGrriIAD1qqlw61629rrrb+2eluH3ts61dtBq9ZbtVdbvdpq1dKqtKLgRNU6ATLIKDOECAmEBAhkfn5/7J2QQIAAJznZO9/365XXPvs5+5yzTtoX25X1POsx5xwiIiIiIiJy7BLiHYCIiIiIiEhYKMESERERERGJESVYIiIiIiIiMaIES0REREREJEaUYImIiIiIiMSIEiwREREREZEYUYIlEkNmtt7Mzo13HCIiIiISH0qwRERERLoo/w+DNWbWa7/xhWbmzKx/s7Hb/LHx+117lZnVm9nu/X76dMy3EOlclGCJiIiIdG3rgC83npjZKUBq8wvMzIB/AcqAK1t5j/ecc932+yluz6BFOislWCLtwMySzex+Myv2f+43s2T/uV5m9nczKzezMjN728wS/OduMrPNZrbLzFaa2Tnx/SYiItIF/AH4erPzK4En97vmDKAP8D1gmpkldVBsIoGjBEukffwnMAEYBYwExgP/5T/3H0ARkAMcB/wIcGZ2MnAdMM45lwGcD6zv2LBFRKQLeh/obmZDzCwCfAn4437XXAn8DXjWP7+wA+MTCRQlWCLt46vAHc65EudcKXA73tQKgFogDzjeOVfrnHvbOeeAeiAZGGpmUefceufcmrhELyIiXU1jFes8YAWwufEJM0sDLgeeds7VAtM5cJrgBH9mRuOP7l/SZSnBEmkffYANzc43+GMAdwOrgdlmttbMbgZwzq0Gvg/cBpSY2TNaICwiIh3kD8BXgKs4cHrgF4A6YKZ//hQw2cxyml3zvnMus9nPCe0dsEhnpQRLpH0UA8c3O+/nj+Gc2+Wc+w/n3EDgIuCGxrVWzrmnnXOn+691wC86NmwREemKnHMb8JpdTAH+ut/TVwLdgI1mtgX4MxClWWMMEdlHCZZI+/gT8F9mluO3vv0J/nx2M7vQzE70OzLtxJsaWG9mJ5vZ2X4zjCpgr/+ciIhIR/gGcLZzrrLZWD5wDt6aq1HsW1v8C1rvJijS5SXGOwCRkPop0B1Y7J//2R8DGAT8Bq/JxQ7gIefcHDMbAfwcGIK3Tutd4JqODFpERLqug6z7PQNY6Jyb3XzQzB4A/sPMhvtDnzGz3fu99izn3Nx2CFWkUzNvbb2IiIiIiIgcK00RFBERERERiRElWCIiIiIiIjGiBEtERERERCRGlGCJiIiIiIjESNy6CPbq1cv1798/Xh8vIiJxNH/+/G3OuZzDX9m5TJo0yW3bti3eYYiISBzMnz9/lnNu0uGui1uC1b9/f+bNmxevjxcRkTgysw3xjuFo6d4lItI1eVuYHp6mCIqIiLSRqlciIl1ar7Zc1KYEy8wmmdlKM1ttZjcf5JorzGyZmS01s6ePJFIREZEjYWaZZjbdzFaY2XIz+4yZZZvZq2a2yj9m+deamT3g38MWm9mYZu9zpX/9KjO7Mn7fSEREwuKwCZaZRYAHgcnAUODLZjZ0v2sGAbcApznnhgHfb4dYRUREGv0aeMU5NxgYCSwHbgZed84NAl73z8G7fw3yf64BHgYws2zgVuBUYDxwa2NSJiIicrTaUsEaD6x2zq11ztUAzwBT97vmW8CDzrkdAM65ktiGKSIi4jGz7sDngN8DOOdqnHPlePemJ/zLngAu8R9PBZ50nveBTDPLA84HXnXOlfn3r1eBwy5eFhEROZS2JFj5wKZm50X+WHMnASeZ2T/N7H0z0w1KRETay0CgFHjczBaY2e/MLB04zjn3KYB/zPWvP9h9rC33N8zsGjObZ2bzSktLY/9tREQkVNqSYLXWLsPtd56IN/XiTODLwO/MLPOAN9JNSkREjl0iMAZ42Dk3Gqhk33TA1hzsPtaW+xvOuUecc4XOucKcnMB1lhcRkQ7WlgSrCOjb7LwAKG7lmhedc7XOuXXASryEqwXdpEREJAaKgCLn3Af++XS8hGurP/UP/1jS7PrW7mNtub+JiIgckbYkWHOBQWY2wMySgGnAjP2ueQE4C8DMeuFNGVwby0BFREQAnHNbgE1mdrI/dA6wDO/e1NgJ8ErgRf/xDODrfjfBCUCFP4VwFjDRzLL85hYT/TEREZGjdtiNhp1zdWZ2Hd5NJwI85pxbamZ3APOcczPYd5NaBtQDP3TObW/PwEVEpEv7f8BT/h/+1gJX4/3R8Dkz+wawEbjcv3YmMAVYDezxr8U5V2Zmd+L9IRHgDudcWUcE75xr84aVIiISLObcAdPNO0RhYaGbN2/eUb9+aXEFlz70Lg9+ZQznDj0uhpGJiEh7M7P5zrnCeMdxpI713gVw+i/e4OzBudwxdXiMohIRkY7Q1ntXmzYa7oyikQSq6xqoqquPdygiIiJtlphg7NhTG+8wRESknQQ2wUpJjABQVdsQ50hERETaLjMtifI9NfEOQ0RE2klwE6yoF3pVrSpYIiISHFlpUXYowRIRCa3AJljJfgWruk4VLBERCY6stCTKNUVQRCS0gptgqYIlIiIB1CMtqgRLRCTEgptgJXqhq4IlIiJBkpWWxO7qOmp0/xIRCaXAJlhmRnJiAtWqYImISIBkpUUBqNirKpaISBgFNsECr4qlCpaIiARJj7QkAHUSFBEJqUAnWCnRiNZgiYhIoDRWsLQXlohIOAU6wUqOqoIlIiLBkuVXsNSqXUQknAKdYKUkqoIlIiLB0iPVX4OlCpaISCgFOsFSBUtERIImK10VLBGRMAt0gqUKloiIBE16UoRoxLQGS0QkpAKdYKmCJSIiQWNm9EhNomKvKlgiImEU6ARLFSwREQmirLQoOypVwRIRCaNAJ1iqYImISBBlpSVpDZaISEgFOsFSBUtERIKoR1qUir2qYImIhFGgEyxVsEREJIiy0qKqYImIhFSwEyxVsEREJIC8KYK1OOfiHYqIiMRYoBOslGiE6lpVsEREJFgy05KoqWtgr/5IKCISOoFOsJITE6ipb6ChQX8BFBHpSsxsvZl9bGYLzWyeP3abmW32xxaa2ZRm199iZqvNbKWZnd9sfJI/ttrMbu6o+DPTogCUay8sEZHQSYx3AMciJRoBoLqugdSkSJyjERGRDnaWc27bfmP3OefuaT5gZkOBacAwoA/wmpmd5D/9IHAeUATMNbMZzrll7Rw3WX6CtWNPDX0yU9v740REpAMFvoIFUF2nKRYiInJQU4FnnHPVzrl1wGpgvP+z2jm31jlXAzzjX9vuMtOSAFWwRETCKNAJVmMFq0rrsEREuhoHzDaz+WZ2TbPx68xssZk9ZmZZ/lg+sKnZNUX+2MHGWzCza8xsnpnNKy0tjUnwmiIoIhJegU6wVMESEemyTnPOjQEmA98xs88BDwMnAKOAT4Ff+ddaK693hxhvOeDcI865QudcYU5OTkyCz/IrWGrVLiISPoFOsFTBEhHpmpxzxf6xBHgeGO+c2+qcq3fONQCP4k0BBK8y1bfZywuA4kOMt7seqY0VLCVYIiJhE+gESxUsEZGux8zSzSyj8TEwEVhiZnnNLvsCsMR/PAOYZmbJZjYAGAR8CMwFBpnZADNLwmuEMaMjvkNKNEJqNKIpgiIiIRSKLoKqYImIdCnHAc+bGXj3saedc6+Y2R/MbBTeNL/1wLcBnHNLzew5YBlQB3zHOVcPYGbXAbOACPCYc25pR32JrLQoO5RgiYiETqATrOSoKlgiIl2Nc24tMLKV8X85xGvuAu5qZXwmMDOmAbZRZlqSpgiKiIRQoKcIpiSqgiUiIsGUmRZVkwsRkRAKdIKlCpaIiARVVloS5Xs1RVBEJGwCnWCpgiUiIkGVmRZVkwsRkRAKdIKlCpaIiARVlr8Gq6HhgK23REQkwNqUYJnZJDNbaWarzezmVp6/ysxKzWyh//PN2Id6IFWwREQkqDLTojQ42FVdF+9QREQkhg7bRdDMIsCDwHl4mzLONbMZzrll+136rHPuunaI8aBUwRIRkaDKTEsCvM2GGzceFhGR4GtLBWs8sNo5t9Y5VwM8A0xt37DapnGjYVWwREQkaLLSvKRKe2GJiIRLWxKsfGBTs/Mif2x/XzSzxWY23cz6tvZGZnaNmc0zs3mlpaVHEe4B70dyYgLVtapgiYhIsGT6CZb2whIRCZe2JFjWytj+K3L/BvR3zo0AXgOeaO2NnHOPOOcKnXOFOTk5RxbpQSQnJlBdpwqWiIgEy74pgqpgiYiESVsSrCKgeUWqAChufoFzbrtzrto/fRQYG5vwDi8lGqFKFSwREQmYLD/B0mbDIiLh0pYEay4wyMwGmFkSMA2Y0fwCM8trdnoxsDx2IR5aclQVLBERCZ7uKV6fKa3BEhEJl8N2EXTO1ZnZdcAsIAI85pxbamZ3APOcczOA75rZxUAdUAZc1Y4xt5CSqAqWiIgET2Ikge4piVSogiUiEiqHTbAAnHMzgZn7jf2k2eNbgFtiG1rbqIIlIiJBlZWepAqWiEjItGmj4c5MFSwREQmqzNSo1mCJiIRM4BMsVbBERCSoMtOSqNirCpaISJgEPsFSBUtERIIqK00VLBGRsAl8gqUKloiIBFVmWhLllapgiYiESeATLFWwREQkqDLTouyqrqO2Xn8oFBEJi8AnWKpgiYhIUDVuNqx1WCIi4RH8BEsVLBERCZKty6BsLeBVsADKtQ5LRCQ0gp9gqYIlItLlmNl6M/vYzBaa2Tx/LNvMXjWzVf4xyx83M3vAzFab2WIzG9Psfa70r19lZld2SPBPXgz//DXgrcECtBeWiEiIBD7BSkmMUFPXQEODi3coIiLSsc5yzo1yzhX65zcDrzvnBgGv++cAk4FB/s81wMPgJWTArcCpwHjg1sakrF1l9IZdWwCviyBAuRIsEZHQCHyClRz1voKqWCIiXd5U4An/8RPAJc3Gn3Se94FMM8sDzgdedc6VOed2AK8Ck9o9yow82FkM7FuDpVbtIiLhEfgEKyUxAkB1ndZhiYh0IQ6YbWbzzewaf+w459ynAP4x1x/PBzY1e22RP3aw8RbM7Bozm2dm80pLS4898oy8pgpWD63BEhEJncR4B3CsGitYVbWqYImIdCGnOeeKzSwXeNXMVhziWmtlzB1ivOWAc48AjwAUFhYe+3z0jDyoLIX6WjKSE0lMME0RFBEJEVWwREQkcJxzxf6xBHgebw3VVn/qH/6xxL+8COjb7OUFQPEhxttXRm/Awe4SzIzMtKiaXIiIhEjwE6yol2CpgiUi0jWYWbqZZTQ+BiYCS4AZQGMnwCuBF/3HM4Cv+90EJwAV/hTCWcBEM8vym1tM9MfaV0aed9z1KQA9UqOaIigiEiLBnyKY2NjkQhUsEZEu4jjgeTMD7z72tHPuFTObCzxnZt8ANgKX+9fPBKYAq4E9wNUAzrkyM7sTmOtfd4dzrqzdo+/eMsHKSktSkwsRkRAJfIKlCpaISNfinFsLjGxlfDtwTivjDvjOQd7rMeCxWMd4SE0VLL9Ve3oSG7ZXdmgIIiLSfgI/RXBfm3ZVsEREJADSekFCYlOr9vzMVIrLq/DyQBERCbrAJ1iNTS5UwRIRkUBISIBu+zYbLshKZXd1HRV71ehCRCQMAp9gqYIlIiKBk9G7aQ1WQVYqAEU79sYzIhERiZHAJ1iqYImISOBk7Ktg5WemAUqwRETCIvAJlipYIiISOBl5sMtbg7WvgrUnnhGJiEiMBD7BUgVLREQCp3seVFVAzR4y06KkJ0VUwRIRCYnAJ1iqYImISOA0tmrfvQUzoyArTQmWiEhIBD/B8jcaVgVLREQCI6O3d2zWSXBzuRIsEZEwCHyCZWYkJSaogiUiIsGR0cc77ty3DktrsEREwiHwCRZASmIC1apgiYhIUOxXwcrPSmVXlfbCEhEJg1AkWMnRCFW1qmCJiEhApPSAxNRme2E1tmpXFUtEJOhCkWClRBOorlMFS0REAsKs1c2GN6vRhYhI4IUiwUpOVAVLREQCpnufZk0utNmwiEhYhCLBUgVLREQCp1kFKystSpr2whIRCYVwJFiqYImISNBk5HkVLOf8vbDUSVBEJAxCkWAlq4IlIiJBk9EbavdAVQUA+ZmpqmCJiIRAKBIsVbBERCRwMvK8Y7N1WNpsWEQk+NqUYJnZJDNbaWarzezmQ1x3mZk5MyuMXYiHpwqWiIgETlOCta+TYMXeWnZWaS8sEZEgO2yCZWYR4EFgMjAU+LKZDW3lugzgu8AHsQ7ycFTBEhGRwNlvs+HGToJq1S4iEmxtqWCNB1Y759Y652qAZ4CprVx3J/BLoCqG8bWJKlgiIhI4TRWsYmDfXlhahyUiEmxtSbDygU3Nzov8sSZmNhro65z7+6HeyMyuMbN5ZjavtLT0iIM9GO2DJSLS9ZhZxMwWmNnf/fP/M7N1ZrbQ/xnlj5uZPeBPc19sZmOavceVZrbK/7myQ79AUhqk9GhWwWpMsNRJUEQkyBLbcI21MuaanjRLAO4DrjrcGznnHgEeASgsLHSHubzNVMESEemSvgcsB7o3G/uhc276ftdNBgb5P6cCDwOnmlk2cCtQiHdfm29mM5xzO9o98kYZeU1rsLLTk0iJJmiKoIhIwLWlglUE9G12XgAUNzvPAIYDc8xsPTABmNGRjS5SEiPU1DXQ0BCznE1ERDoxMysALgB+14bLpwJPOs/7QKaZ5QHnA68658r8pOpVYFK7Bd2ajN6w00uwvL2w0jRFUEQk4NqSYM0FBpnZADNLAqYBMxqfdM5VOOd6Oef6O+f6A+8DFzvn5rVLxK1Ijnpfo6ZeVSwRkS7ifuBGYP9/+O/ypwHeZ2bJ/tjBprofdgo8tN/0dgAy+jRNEQRvmmBRuaYIiogE2WETLOdcHXAdMAtvKsZzzrmlZnaHmV3c3gG2RUpiBEDrsEREugAzuxAocc7N3++pW4DBwDggG7ip8SWtvI07xHjLAececc4VOucKc3Jyjj7w1mT0ht1boMHLEwuytNmwiEjQtWUNFs65mcDM/cZ+cpBrzzz2sI5MYwVL67BERLqE04CLzWwKkAJ0N7M/Oue+5j9fbWaPAz/wzw821b0IOHO/8TntGPeBMvKgoQ72bIduORRkpVG+p5bd1XV0S27TLVpERDqZNm003NmpgiUi0nU4525xzhX409KnAW84577mr6vCzAy4BFjiv2QG8HW/m+AEoMI59ynezIyJZpZlZlnARH+s43RvvVW7Gl2IiARXKP481ljBqqpVBUtEpAt7ysxy8Kb+LQT+zR+fCUwBVgN7gKsBnHNlZnYn3lpjgDucc2UdGnHTXlhbIG8k+Zn7WrWf3DujQ0MREZHYCEWC1VjBqq5TBUtEpCtxzs3Bn9bnnDv7INc44DsHee4x4LF2Cu/wMnp7R79Ve0FWGqDNhkVEgiwUUwRVwRIRkUDqdhxgTa3ae3VLIjkxQZsNi4gEWCgSrJSoKlgiIhJAkSik5zRVsLy9sNRJUEQkyEKRYCUnqoIlIiIBldF7v72w0thcrgRLRCSoQpFgqYIlIiKBlZEHO4ubTvNVwRIRCbRwJFhNbdpVwRIRkYDJ6g871oPz9jjul51GWWUNFXtr4xqWiIgcnVAkWPs2GlYFS0REAiZ7ANTs8jYbBobkdQdgWfHOeEYlIiJHKRQJlipYIiISWFkDvGPZWgCG9fESrKXFFfGKSEREjkEoEixVsEREJLCyB3rHsnUA9OqWTO/uKSxVBUtEJJDCkWCpi6CIiARV1vGAwY51TUPD87uzZLMqWCIiQZQY7wBiwcxISkxQBUtEjkltbS1FRUVUVVXFO5TQSElJoaCggGg0Gu9QOq/EZOhR0DRFEGBonx68saKEvTX1pCZF4hiciASB7l+xdaz3rlAkWAApiQlUq4IlIsegqKiIjIwM+vfvj5nFO5zAc86xfft2ioqKGDBgQLzD6dyy+jdNEQQY3qc7DQ6Wb9nJmH5Z8YtLRAJB96/YicW9KxRTBAGSoxFVsETkmFRVVdGzZ0/dnGLEzOjZs6f+otoW2QNaTBEclt8DQOuwRKRNdP+KnVjcu0KTYKVEE7QGS0SOmW5OsaXfZxtlD4TKUqjeBUCfHilkpUVZqnVYItJG+vc2do71dxmaBCs5MUJVrSpYIiISQE2t2r0qlpkxrE8PVbBERAIoNAlWSjSB6jpVsEQk2MrLy3nooYeO+HVTpkyhvLz8kNf85Cc/4bXXXjva0KQ9ZfsJVotpgt1ZuWUXNbq3iUgnp3tXS6FJsFTBEpEwONhNqr7+0P++zZw5k8zMzENec8cdd3DuueceU3zSTvbbbBhgWJ8e1NQ3sKpkV5yCEhFpG927WgpNgqUKloiEwc0338yaNWsYNWoU48aN46yzzuIrX/kKp5xyCgCXXHIJY8eOZdiwYTzyyCNNr+vfvz/btm1j/fr1DBkyhG9961sMGzaMiRMnsnfvXgCuuuoqpk+f3nT9rbfeypgxYzjllFNYsWIFAKWlpZx33nmMGTOGb3/72xx//PFs27atg38LXVBKd0jrdUAnQVCjCxHp/HTvaik0bdqTEyOU76mNdxgiEhK3/20py2L8H7ZD+3Tn1ouGHfKan//85yxZsoSFCxcyZ84cLrjgApYsWdLUKvaxxx4jOzubvXv3Mm7cOL74xS/Ss2fPFu+xatUq/vSnP/Hoo49yxRVX8Je//IWvfe1rB3xWr169+Oijj3jooYe45557+N3vfsftt9/O2WefzS233MIrr7zS4kYo7Sx7QIsKVv+e6aQnRbxGF4V94xiYiARJPO5fune1pAqWiEgnNn78+Bb7cDzwwAOMHDmSCRMmsGnTJlatWnXAawYMGMCoUaMAGDt2LOvXr2/1vS+99NIDrnnnnXeYNm0aAJMmTSIrS3swdZjsgbBjfdNpQoIxtE93VbBEJHC6+r0rVBUsrcESkVg5XKWpo6Snpzc9njNnDq+99hrvvfceaWlpnHnmma3u05GcnNz0OBKJNE2zONh1kUiEuro6wNtgUeIkawAsfg7qqiHR+99mWJ8ePDdvE/UNjkiCWjCLyOF1hvtXV793qYIlItKJZGRksGtX600NKioqyMrKIi0tjRUrVvD+++/H/PNPP/10nnvuOQBmz57Njh07Yv4ZsWJmETNbYGZ/988HmNkHZrbKzJ41syR/PNk/X+0/37/Ze9zij680s/Pj80182QMABzs2NA0N69OdPTX1rN9eGb+4REQOQ/euloKbYG1fA898FTbPB1TBEpFw6NmzJ6eddhrDhw/nhz/8YYvnJk2aRF1dHSNGjODHP/4xEyZMiPnn33rrrcyePZsxY8bw8ssvk5eXR0ZGRsw/J0a+Byxvdv4L4D7n3CBgB/ANf/wbwA7n3InAff51mNlQYBowDJgEPGRmkQ6K/UDZA71js1btw/N7ALBEGw6LSCeme1dLFq+SWmFhoZs3b97Rv0HpJ/DgOLj0URhxBT97eTmP/3M9n/x0cuyCFJEuZfny5QwZMiTeYcRVdXU1kUiExMRE3nvvPa699loWLlx4TO/Z2u/VzOY75wqP9j3NrAB4ArgLuAG4CCgFejvn6szsM8BtzrnzzWyW//g9M0sEtgA5wM0Azrmf+e/ZdN3BPveY712HsrsU7jkRJv0CJvwbALX1DQy7dRZXf7Y/t0zp2v/fFJGD6+r3r8527wruGqxuOd6xshSAlMQINXUNNDQ4EjRPXUTkqGzcuJErrriChoYGkpKSePTRR+Md0sHcD9wINP6JsidQ7pyr88+LgHz/cT6wCcBPvir86/OB5nNVmr+miZldA1wD0K9fv9h+i+bSe0FStxadBKORBAb3zmBJsSpYIiIH09nuXcFNsFIyISEKu0sASI56sx1r6htISYjfDA8RkSAbNGgQCxYsiHcYh2RmFwIlzrn5ZnZm43Arl7rDPHeo1+wbcO4R4BHwKlhHHHBbmXnrsJpNEQRvHdbLS7bgnMNMf0AUEdlfZ7t3BXcNlhmk50Clt4lYSqKXVGkdlohI6J0GXGxm64FngLPxKlqZ/hRAgAKg2H9cBPQF8J/vAZQ1H2/lNfGR1XIvLPA6CZbvqWVzeesdtUREpHMJboIF3nSKypYVLHUSFBEJN+fcLc65Audcf7wmFW84574KvAlc5l92JfCi/3iGf47//BvOW4A8A5jmdxkcAAwCPuygr9G67IFeF8GGfX8sbGx08XGRpgmKiARBsBOsbrkt1mCBKlgiIl3YTcANZrYab43V7/3x3wM9/fEb2NfcYinwHLAMeAX4jnMuvjeR7AHQUAs7NzcNDcnLIBoxFinBEhEJhOCuwQJvimDJCkAVLBGRrsg5NweY4z9eC4xv5Zoq4PKDvP4uvE6EnUPWAO9YthYyvYYayYkRhuZ1Z9Gm8jgGJiIibdWmCpaZTfI3YVxtZje38vy/mdnHZrbQzN7x9xZpf+k5XgXLOVWwRKRL6tatGwDFxcVcdtllrV5z5plncrjW4vfffz979uxpOp8yZQrl5foP+g7XuBdWWctGFyP7ZvLx5grqG+KztYqISCyF/d512ATL33TxQWAyMBT4cisJ1NPOuVOcc6OAXwL3xjzS1qTnQH01VO9sqmBV1aqCJSJdT58+fZg+ffpRv37/m9TMmTPJzMyMRWhyJLr3gUjSAZ0ERxZksru6jrWlu+MUmIhI7IX13tWWCtZ4YLVzbq1zrgavY9PU5hc453Y2O02nlTa37aJbrnes3EZK1KtgVdepgiUiwXXTTTfx0EMPNZ3fdttt3H777ZxzzjmMGTOGU045hRdffPGA161fv57hw4cDsHfvXqZNm8aIESP40pe+xN69+7rPXXvttRQWFjJs2DBuvfVWAB544AGKi4s566yzOOusswDo378/27Z5XVrvvfdehg8fzvDhw7n//vubPm/IkCF861vfYtiwYUycOLHF58hRSohA5vEHdBIc2df7D4aFmiYoIp2Q7l0ttWUNVtMGjb4i4NT9LzKz7+AtHk7Ca5l7gJhv1pjeyzvuLiHDueDqAAAgAElEQVQ5sSegCpaIxMjLN8OWj2P7nr1Pgck/P+Ql06ZN4/vf/z7//u//DsBzzz3HK6+8wvXXX0/37t3Ztm0bEyZM4OKLLz7onkgPP/wwaWlpLF68mMWLFzNmzJim5+666y6ys7Opr6/nnHPOYfHixXz3u9/l3nvv5c0336RXr14t3mv+/Pk8/vjjfPDBBzjnOPXUU/n85z9PVlYWq1at4k9/+hOPPvooV1xxBX/5y1/42te+doy/JCF7IJStbzE0sFc6GcmJLCoq5/LCvq2/TkQE4nL/0r2rpbZUsNq6EeODzrkT8Lo4/Vdrb+Sce8Q5V+icK8zJyTmySFuT3ljBKlUFS0RCYfTo0ZSUlFBcXMyiRYvIysoiLy+PH/3oR4wYMYJzzz2XzZs3s3Xr1oO+x1tvvdV0sxgxYgQjRoxoeu65555jzJgxjB49mqVLl7Js2bJDxvPOO+/whS98gfT0dLp168all17K22+/DcCAAQMYNWoUAGPHjmX9+vXH+O0F2LfZcLNW7QkJxoi+PVi0SZ0ERaTz0b2rpbZUsI50I8ZngIePJag2S/eTtMpSko/TGiwRiaHDVJra02WXXcb06dPZsmUL06ZN46mnnqK0tJT58+cTjUbp378/VVVVh3yP1v5CuG7dOu655x7mzp1LVlYWV1111WHfx9suqnXJyclNjyORiKYIxkr/M+CD38Lbv4LP39g0PLIgk0feWktVbX3THxVFRA4Qp/uX7l37tKWCNRcYZGYDzCwJb1PHGc0vMLNBzU4vAFbFLsRDaJwiqAqWiITItGnTeOaZZ5g+fTqXXXYZFRUV5ObmEo1GefPNN9mwYcMhX/+5z32Op556CoAlS5awePFiAHbu3El6ejo9evRg69atvPzyy02vycjIYNeuXa2+1wsvvMCePXuorKzk+eef54wzzojht5UDDL4ATrkc5vwcNn7QNDyybyZ1DY5ln+48xItFROJD9659DlvBcs7Vmdl1wCwgAjzmnFtqZncA85xzM4DrzOxcoBbYAVzZnkE3iUQhNcurYCWqgiUi4TBs2DB27dpFfn4+eXl5fPWrX+Wiiy6isLCQUaNGMXjw4EO+/tprr+Xqq69mxIgRjBo1ivHjva2hRo4cyejRoxk2bBgDBw7ktNNOa3rNNddcw+TJk8nLy+PNN99sGh8zZgxXXXVV03t885vfZPTo0ZoO2J7M4IJ7YdOH8Jdvwr+9DamZjPIbXSzaVM6YfllxDlJEpCXdu/axQ5XQ2lNhYaE7XG/7NvnNeMg5mapL/4/BP36FGyedzL+feeKxv6+IdDnLly9nyJAh8Q4jdFr7vZrZfOdcYZxCOmoxu3e1xaa58Nj5MHQqXPYYmDHhv19nwsBs7p82umNiEJFA0P0r9o7l3tWmjYY7tfQcqNymCpaIiIRL33Fw1o9g6V9hoTdtZmTfHiwqUqMLEZHOLPgJVrccqCzBzOiWnMjOvbXxjkhERCQ2Tr/ea3ox80bYWczIvpms21ZJ+Z6aeEcmIiIHEfwEKz0HKksBKMhKpWjHnsO8QETk4OI1bTqs9Ps8RgkROP8uqK2EDe8yqsBbh7VYVSwR2Y/+vY2dY/1dhiDByoWqCqirpiArjU1lahMsIkcnJSWF7du36yYVI845tm/fTkpKSrxDCbacwWARKFnO8IIemHmNLkREGun+FTuxuHe1ZR+szq2pVfs2+man8u6abTjnDrpLtIjIwRQUFFBUVERpaWm8QwmNlJQUCgoK4h1GsCUmQ88ToWQ53VOinJDTjUVFSrBEZB/dv2LrWO9dwU+wuuV6x8oS+mVnsqemnu2VNfTqlnzo14mI7CcajTJgwIB4hyFyoNwh8OkiwNtw+B+flOqPiSLSRPevziUEUwRzvGPlNvpmpQGwqUzrsEREJERyh8CO9VCzh1F9e7BtdzXFFVXxjkpERFoRngRrdwl9s/0Ea4fWYYmISIjkDgEcbFvJSH/D4YUbNU1QRKQzCk+CVVlKQVYqoAqWiIiETO5Q71iynMG9u5OWFOG9tdviG5OIiLQq+AlWcjeIpkFlKenJifTqlqQES0REwiVrAESSoGQ5SYkJnDGoF28sL1HHMBGRTij4CRZ4nQSb9sJKY5P2whIRkTCJJEKvk6FkOQDnDD6O4ooqln+6K86BiYjI/kKSYOXC7hIA+mZrLywRkTAzsxQz+9DMFpnZUjO73R//PzNbZ2YL/Z9R/riZ2QNmttrMFpvZmGbvdaWZrfJ/rozXd2qT3CFQugKAMwd70+PfWLE1nhGJiEgrQpJg5UClNxe9b1YqxeV7qatviHNQIiLSTqqBs51zI4FRwCQzm+A/90Pn3Cj/Z6E/NhkY5P9cAzwMYGbZwK3AqcB44FYzy+rA73FkcgdDxSao2kluRgoj+2by2vKSeEclIiL7CUeC1S0HKvdVsOoaHJ+qfa2ISCg5z27/NOr/HGox0lTgSf917wOZZpYHnA+86pwrc87tAF4FJrVn7MeksdGFX8U6Z3Aui4rKKd1VHcegRERkf+FIsBorWA0N9Gtq1a51WCIiYWVmETNbCJTgJUkf+E/d5U8DvM/MGneczwc2NXt5kT92sPH9P+saM5tnZvNKS0tj/l3aLHeId/TXYZ09OBfnYM5KVbFERDqTkCRYueDqYe+Ops2Gi7QOS0QktJxz9c65UUABMN7MhgO3AIOBcUA2cJN/ubX2FocY3/+zHnHOFTrnCnNycmIS/1Hp0c/rmusnWMP6dKd39xRe1zRBEZFOJSQJVi/vWFlKXmYKCaYKlohIV+CcKwfmAJOcc5/60wCrgcfx1lWBV5nq2+xlBUDxIcY7p4QEyBkMJcsAMDPOHpLL26tKqa6rj3NwIiLSKBwJVrdc71hZQjSSQF6PVDZqLywRkVAysxwzy/QfpwLnAiv8dVWYmQGXAEv8l8wAvu53E5wAVDjnPgVmARPNLMtvbjHRH+u8coc2rcECbx1WZU09H64ri2NQIiLSXDgSrHR/yoa/F1bf7FRtNiwiEl55wJtmthiYi7cG6+/AU2b2MfAx0Av4qX/9TGAtsBp4FPh3AOdcGXCn/x5zgTv8sc4rdzDs3gqV2wE47cRepEQTNE1QRKQTSYx3ADGR7lewdnsJVr/sNN5cGceFyCIi0m6cc4uB0a2Mn32Q6x3wnYM89xjwWEwDbE+NjS5Kl0P66aREI5x2Qi9eX7GVWy8aile8ExGReApHBSs1Cyyyr4KVlUbprmr21mhOuoiIhEhjq3a/0QXA2UNy2VS2l1Uluw/yIhER6UjhSLASErxGF01TBP1Ogmp0ISIiYZKRB8k9WiRY5ww+DoDXlm+NV1QiItJMOBIs8PfC2rcGC9RJUEREQsbMmybYLMHq3SOFscdn8ds5a1hdsiuOwYmICIQ2wfI3G9ZeWCIiEja5Q7w1WG7fll33f2kUSYkJXPX4XEp3VccxOBERCVeCtdvropTTLZmUaII6CYqISPjkDoG9O7xugr6+2Wn8/spxbNtdzTefnKc1yCIicRSeBKtbLlRuA7zNFwuy0rQXloiIhE9jJ8Fm0wQBRvbN5IFpo1lcVM73nllAfYNr5cUiItLewpNgpfeC2kqoqQSgb1Yqm3ZoiqCIiIRMYyfB9W8f8NTEYb35yYVDmb1sKz+bufyA50VEpP2FKMHy98Jqtg6rqGwPzukveCIiEiLpvWDYF+Dd38D2NQc8ffVpA/iXCcfzu3fWsWDjjjgEKCLStYUowcrxjs02G95VXUfF3to4BiUiItIOzv8ZRJJg5g9aNLtodNPkweRmJHPbjKU0aKqgiEiHCk+C1c1PsPwKVkGW10lQ67BERCR0uufBOT+GNW/A0r8e8HS35ERumTKYRUUVTP+oKA4Bioh0XeFJsBorWJVeJ8GmvbDUql1ERMJo3DchbxS8cgvsLT/g6UtG5TOmXya/fGUFO6s0m0NEpKOEL8Havd9eWNpsWEREwighAhfd783ceOOnBzxtZtwxdTjbK2v49Wur4hCgiEjXFJ4EKzEZMvJg2ycAdE+JkpkW1V5YIiISXn1Gw/hrYO7voGj+AU8Pz+/BtHH9eOLd9azauisOAYqIdD1tSrDMbJKZrTSz1WZ2cyvP32Bmy8xssZm9bmbHxz7UNsgfC5v33WD6ZaexfntlXEIRERHpEGf9J2T0hpeuh4YDNxj+wcSTSEuKcONfFvPbf6zh3lc/4b9nLueul5ZRvqcmDgGLiITbYRMsM4sADwKTgaHAl81s6H6XLQAKnXMjgOnAL2MdaJvkj4GyNbCnDICRBZks2FhObX1DXMIRERFpdynd4fz/hk8XwbzHDni6Z7dkfjRlCIs2lfPzl1fwwOurePK99Tz69jqem7ep4+MVEQm5tlSwxgOrnXNrnXM1wDPA1OYXOOfedM41zsV7HyiIbZhtlD/WOxYvAOCzJ/RkT009izYduPhXREQkNIZ9AQaeCa/fCbu2HvD0tPH9WHTrRJbfMYm1/z2FFXdOZmhed2YtPfBaERE5Nm1JsPKB5n/iKvLHDuYbwMutPWFm15jZPDObV1pa2vYo26rPaO9Y/BEAEwb2xAzeXbM99p8lIiLSWZjBlF9B3V549cetXpKREiU1KUJCggFw/rDefLRxByW7qjoyUhGR0GtLgmWtjLW6a6GZfQ0oBO5u7Xnn3CPOuULnXGFOTk7bo2yrlB7Q6yTY7CVYWelJDM3rzj9Xb4v9Z4mIiHQmvU6E074Hi5+FdW8f9vLzhx+Hc/DqMlWxRERiqS0JVhHQt9l5AVC8/0Vmdi7wn8DFzrnq2IR3FPLHQtG8pp3tTzuxFws2lrO35sCFvyIiIqFy+g2Q2Q9e+g+oO3QDi5OPy+D4nmnhmyZYsRn+cTc0aP21iMRHWxKsucAgMxtgZknANGBG8wvMbDTwv3jJVUnswzwC+WO9zYZ3bgbgMyf0pKa+gXkbyuIaloiISLtLSoPJd8O2lfDyjV7Ti/cegrfugQV/bJF0mBnnD+vNe2u2hWsj4sXPwJs/9X4HIiJxcNgEyzlXB1wHzAKWA88555aa2R1mdrF/2d1AN+DPZrbQzGYc5O3aX/4Y7+i3ax/fP5vEBNM6LBGRkDCzFDP70MwWmdlSM7vdHx9gZh+Y2Soze9b/oyBmluyfr/af79/svW7xx1ea2fnx+UYxdvIkGHoJzH8c/n49zLoF3rgTXvwOLPxji0vPH3YctfWON1fE92+jMbV9rXfcujS+cYhIl5XYloucczOBmfuN/aTZ43NjHNfRO244RJK8BGvoVNKTExnVN1MJlohIeFQDZzvndptZFHjHzF4GbgDuc849Y2a/xWu69LB/3OGcO9HMpgG/AL7kbzkyDRgG9AFeM7OTnHPBn1N+2eNQfhskpkA01Tv+8VKY/WM4aTJ089ZBj+6bRU5GMrOXbmXqqEP1rwqQsjXesWR5fOMQkS6rTRsNB0piMvQ+panRBcBnT+zFx0XlVOwN0RQIEZEuynl2+6dR/8cBZ+PtxQjwBHCJ/3iqf47//DlmZv74M865aufcOmA13tYkwZeQANkDoHsepGZCNAUuvA9qKmH2fza7zDhv6HHMWVlCVW3w80oAtivBEpH4Cl+CBd46rOIFTTvaf/aEnjQ4+HCd1mGJiISBmUXMbCFQArwKrAHK/Wnt0HJLkabtRvznK4CetHEbknbfYqSj5JwMp1/vdRlc82bT8PnDelNZU3/Yjrubyvbw+vJO3hCjepe3DhugRFMERSQ+wptg1eyGbZ8AMLpfJinRBLVrFxEJCedcvXNuFF5n2/HAkNYu848H226kTduQtPsWIx3pjP+A7IHw0g1QuxeAzwzsSUZKIrOWbjnoy5xzfP/ZhXzryXmU7opfo+DDKvPXXx13CuxY71XsREQ6WHgTLGhqdJGcGGFc/2ze0zosEZFQcc6VA3OACUCmmTWuLW6+pUjTdiP+8z2AMtq4DUmoNE4VLFsLb/8KgKTEBM4enMtry0uoq2+9tfnry0uYv2EHDQ5eWtyJf0WN0wOHXOgdS1fELxYR6bLCmWBlnwDJPZoSLIDPntCLlVt3de6/vImIyGGZWY6ZZfqPU4Fz8brcvglc5l92JfCi/3iGf47//BvOOeePT/O7DA4ABgEfdsy3iKOBZ8KIL8E798OWJQBMHNqbssoa5m/YccDl9Q2OX85awYBe6QzuncGLizpxgtXY4GLwBd5x67L4xSIiXVY4E6yEBMgfvV+C1ROA99aqiiUiEnB5wJtmthhvr8ZXnXN/B24CbjCz1XhrrH7vX/97oKc/fgNwM4BzbinwHLAMeAX4Tig6CLbFxLsgLRue+zpUVXDmyTmkRBP41exPDmh28cKCzXyydTc/mHgyXxidz4KN5Wzcvqd94tpdAn++GiqP8l69fS1k5EHuMEhMVaMLEYmLcCZY4E0T3Lq0aY758PweZKQk8t4arcMSEQky59xi59xo59wI59xw59wd/vha59x459yJzrnLnXPV/niVf36i//zaZu91l3PuBOfcyc65l+P1nTpctxy4/Ako3wDPX0t6NIFffHEEH64v4/pnF1Lf4C1Fq66r595XP+GU/B5MHt6bi0b2AWDGos3tE9fyGbD0r7By5uGvbU3ZGm8WS0IC5A6GElWwRKTjhTvBaqiDLR8DEEkwJgzsyTurt+HNDBEREenCjv8MnHcnrHwJ/nk/U0fl818XDOHlJVu4bcZSnHM89f5GNpfv5cZJJ5OQYPTJTGX8gGxeWFjcPvfSDe/5x3eP7vXb10DPgd7j3KFKsEQkLsKdYEGLaYJnD85lU9lelmzeGaegREREOpEJ18KwS+GNO2HtHL55xkC+/bmB/OH9Ddw9ayW/eXM1p53YkzMG7eueePHIPqwu2c2yT1u5l9bXtTgt3VXd9v21nNuXWG3455F/l6oK2LPN65IIXoK1e+vRTzcUETlK4U2wMnpD93zY9EHT0JTheSRFEnhhYTtNbRAREQkSM7j4f6DXSTD9X+GT2dw0LsplI3N4aM4ayipruPH8wS1eMuWUPBITjBkL92t2UbwQ7j4Blr4AQFVtPZN//TY3PLewbbGUb4BdxdBzkPe4oujIvktji/bsE7xjrt+5v1TrsESkY4U3wQKvi9CyF71/9IEeaVHOGpzDjEXFB21FKyIi0qUkd4Mv/dGbVv/05SQ8OJa7V05kYbfv8kKfPzCye8u9pLLTk/jcSd69tMFfq0VtFTz/bagqh3/+GoBXlmxh2+5qZn68hYWbyg8fR2P16owb/PP3jux7NLZo79mYYA31juokKCIdLNwJ1ln/Cem5MOP/NU1b+MLofEp3VfOu9sQSERHx9BoE31sEV82ES36LnXkzmUPOZlTFG/CbQvjHL5uaRgFMHdWHTyuqmLu+zBt4405vz6khF0HxR7D5I57+YCP9stPomZ7E3bPasB/VhnchJRNOuRySMo58mmBjBStrgHfM6A2pWVqHJSIdLtwJVmomTPklbFkM7z8IwJkn59I9JVHTBEVERJpLzYL+p8GoL8OZN8MXH4XrPoRB58Gbd8FvxsGSv4JznDvkOFKjEW9PrPX/hPcehLFXw9QHIZpOxdu/5cP1ZXz11H5856wT+efq7byz6jBdfDe+B/0+A5Eo9JvgnR+J7Wu8pQFJad65mRpdiEhchDvBAhhyMZx8Abz5MyhbR0o0wpRT8pi1ZAt7auoO/3oREZGuKqs/XPEkXPWSV12afjX83wWkly3jvKHHMWfxWtwL10LW8TDxp5DSA0ZcTtrKF+gV2cNlYwv46oR+5GemcvesFQfvPLhrK2xf7XU2BDj+s15FrPIItlYpW7OvwUWj3CHeXlgd0T14T5k3VbI9bV0Kn8xq38+Q2Kqvg79+GzZ/FO9IpAOFP8Eygwvu8f4i9vfvg3NcMjqfypp6Xl22Nd7RiYiIdH79T4dv/wMuvM9LWB75PLfU/y8/qHsEyjfCJb/11nIBVaOuJuqq+VGfBfTslkxyYoTvnTuIRUUVzFq6pfX3b6xW9fusdzz+tJbjbbF9zb71V41yh0L1TtjZzrNWGurht6fDrB+17+e8dpvXjKSha+yHHQrbV8PiZ7w93qTLCH+CBdC9D5x7K6ydA4v+xPj+2fTpkcILCzRNUEREpE0SIlD4r/Ddj2D8t8lb8xyXRt7hkboLeLvmxKbL/ra1Jx81nMjk6pebKkeXjs7nxNxu3DP7k9abTG14F6JpkDeSij210Gc0JKa0fT+svTtgb9m+DoKNOqrRxeb5XhK39K9QX9s+n9HQAJs+hJrdXpIrwVDqrz/cvjq+cUiH6hoJFsDYf4W+E2D2f5FQX83U0fm8tWob23dXxzsyERGR4EjNgsk/h2vfpfbs25iRfRXXP7uQkl3e9LinP9zIrLQLSd25Fta9BUBiJIEfTDyJypINvPLPDw98z43vQkEhv3lrA6PunM2slWVQMK7tjS4aG1wcUMHyW8y39zqsxml7e3fA+nfa5zO2r/K6NAIUtfI7lM6pdKV3bOxyKV1C10mwEhLg8zfCnu2wciaXjMqnvsHx98WfxjsyERGR4MkdTPRz13PfVyewu7qOG55dxJLNFSzYWE7eZ74Mqdkw93fetdW7OX/LI7yVcj2nvX4Z7yxYsu999pbDliV8ZEO5Z/YnRCMJ3D5jKTX5E2DLx1DVyobGzeyqqmXrej+B2r+ClZrlNb5o74rPJ7MgfyxE073tYdpD476ekSTYNLd9PkNir7GCVbbWq0JKl9B1EiyAgWd6/9AufJqTe2cwJK87z2uaoIiIyFE76bgMbrtoGO+s3sa3npxHUmICl4wbCKO/Citegg/+F35TiL1zL3UnTSHNqqn+63U8+o81XtOLTR8CjrtX9GTy8N784V/HU1xRxfTt/cD50+IOoqq2ni8/+j5Pz3yTBoxHlzSwuXxvy4tyh0DJ0hZDc1aWNFXcjllFEWz9GIZOhZPOh+V/a581Ups+9BLGE85RBStIGitYdVXtvxZQOo2ulWAlRGDkl2HN67CzmC+M7sPCTeWs3LIr3pGJiIgE1pfG9eXCEXl8WlHFhafkkZmW5K3XcvXw8o2QkQffeI3Ur/wBO/c2zoksYM3sh7hx+mJWfDiLWhch44QJ/HraaE4d2JMrCgv42eJuuITEA6cJNjRAQwPOOX78whKWbN7JucftZltCDnfNXsdpP3+DKx/7kMpqv1Nw7hAo/aRpP8z5G3Zw1eNzueHZRbH58o3TA0+a5CVZe7a1fe3Ykdj0IRSMh77jvPU8e8pi/xkSW/V13tTOPmO886Ctw9qyBIrmxTuKQOpaCRbAqK94fxFb9AyXje1LelKEX7/+SbyjEhERCSwz42eXnsLXP3M83z1nkDeYPRCm3ON1GPzm615iACR99lpc/89xR/JTvP/RfHZ/8hbrkgbxwNdPIynR+8+SmyYNJiG5G6sjJ+KaJyufzIZfj4Bnv8bTH6znz/OL+O7ZJ3JK6jZyjx/CWz88i++dM4h/fFLKA6+v8l6TOwzqq2HHOpxz/PSlZZjBO6u38d6a7Uf1fesbHE+8u95bx71qttfOvtdJ3p5hiamtTxOs2nnIatwh7SmDbSuh73gvyQIo0jTBTm/HeqivgcEXeOdBS7Bm/hD+ek28owikrpdg9TzBawO78Cmy06L86+kDmPnxFpYWV8Q7MhERkcDKSIlyx9Th9O+Vvm9w/Le8jYsTmv3nRkICdslDJEWjzMh7glGRtfQfex4p0UjTJT27JXPjpJN5fe+JuKJ5ULEZ/vItePpyr0vfypfY9NLdnHlyDt8796SmFu39eqZx/XknMW1cX373zjqWf7oT8kZ6bzr397z08acs2FjOHRcPo3f3FO6ZvfLge3MdwuvLt3LrjKX89rUlXofikyZ528IkpXtJ1vK/tVxv09AAf74SHjsfyjcd8ec1VRH6ngr5Y8AiR5+sScdpXH818CyvS2ZjM5YgaGjw1kCWrYEq/Tfykep6CRZ4Vaztq2HTh3zzjIF0T0nkvldVxRIREekQmX1hyt1klS0k0dWRNOD0Ay6ZNq4fpdljSXB1NPzPWNzSv1L/uZso/cZc3rAJ/CDyDP9zRh2Rqh1ed71mDS5umjSYHqlRfvT8xzTkDIFT/w0+eJj5f3+Ewb0z+Mqpx3Pd2Scyf8MO5nxSesTh/+H9DQBsXjDbW1szaOK+J4dOhd1bWq6TmvsorHnDm0Gz+Nkj/jyKPvSSqvwx/7+9+w6PqsweOP59M+m9JxACofcmXRRQiqAoiBVFEcW26rqrrquuvfxWd+1lLSuwYFcEQVREAWlSpHcIkBBCeu+ZzMz7++OdkEIIAQZC4HyeJw/kzp07d25ucufc97znmCAuqmvTn4eVuArKixp7L06vygArooM5P092BEtrk3Z6Jotk5CWC1TmFJtVF6bTnkfMzwOo63txJ2PwpQT4e3HlxG37dlcHmQ3mNvWdCCCHE+aHHDdD5KnDzgJYDjnrY4qaYMO5aCrUP26zNGF36Im0X9WTwq6v4u+1OHAHNCfj+bji80TyhWon2ED9P/nF5ZzYl5fHlH4dg1IukBF3Ao+Xv8c9BZtvX940lNtSH105wFGt/ZhEr4rO4rGsUF9r/oMLiYxoxV+pwGVi8qtIEM3bDL0+bIKzlhbDlyyP9wRrs0FqI7maCKzCpgoc3Nt2Gw3lJ8L/LYdWbjb0np1fmHgiKBa8Ac36ebIC1dyHMGAO7F7h2/+qTVq3SZ8rmM/e654jzM8DyCoAu42H7XLCWMOWi1oT4evDaoj2NvWdCCCHE+UEpmPBfuHu5qY5Xh65tW5J5xx8cumY+k64aw19HdOCmAS15a/IwPG/4HxSmwHf3mJVrlWifcEEMA9uE8vJPu9ibVcZNefdQ6h5I79X3Q0kOnu5u/GV4B0pTdpHwxcOw+j8NGiH4ZPVBPC1uvDS+G5d5bmU1PXG4eVat4BUA7YabAMtWDnPuNIHRVe+adMns+KqgsCHsNkjeYJSOJ6kAACAASURBVNIDK7Xo72w4fJr7e50uB34z/+5d2Ki7cdpl7oaIjub/Ye0g9+DJNaKubHdwJkct07aBcgO/SEiVAOtEnZ8BFpjysdZC2PU9/l7u3DusLSvis1iXIFV5hBBCiDPCwxuiutS7SpuWsYztGcstg+J4cER7nr2qKxe2C4cWfWHEs1CcaT4IhsTVeJ5SihfHd6e0ws41//mdQxUBFI+fDoWpMPt22PoNE7ZMZbHX32i1dwb8/Dh8fn291fmKy218uyGZy7tHE168jwhHJt+X9WBZ7TTDLuNMSe6vJkHaVrjybQiIMjd33b1hy+cNP0YZO6CiuGaA5SwY0mTnYVUGWGnboCClUXfltHHYIWsvRDibXYe1M1U1cw+e2HZyEmDfYvP/w5tcu4/1Sd8OYe3NaGnKGXzdc8T5G2C1Gmz+GG/+FIBbBsYREeB1wqkCQgghhGgkA++DjldAVDdw9zzq4XaR/twztC2F5TYm9o8ltvtQU9nwwFKYMxVVmMaubo/Qv+w9vmvxCLb9S8l4bSC3vPgRo99cTkqtnlpzNx2msNzGLYPijoy+bPftz/RVCTVfuMNok/oYvwh6T4LOY81y70DoNBa2f2tGtxqiMohq0a9qWUhr8A1vmpUEHQ44sAya9TLfV5a5P9fkJZn5eUdGsJwjrCeaJrhhhrmB0GmsCXTOVFpo2jaI7g7Ne5niHFLo4oScvwGWUtDrZkhYDgeW4eNp4b5hbVmbkMNDX29h4fZUiip7aAghhDhrKKVilVJLlVK7lFI7lFIPOpc/q5Q6rJTa7Py6vNpzHldK7VNK7VFKXVZt+Wjnsn1Kqcca4/2IU+DmBjd8AlN/PeYq913SjhfHd+PR0c6RhD6TTWriLXPhgY10nPAkMS1a8pd9FzBZP4ebtjPd/g+G5s7hno8Xk1VkAiGtNbNWJ9ItJpALWgabwKB5by4f2IsV8Vnsy6jWU9MnGDqONqXqR79cc4d6ToTSXBN8NcShdeAfDcEtq5YpZUYWmmKAlb7d9AobcDcEtWz4cWhqKhsMVx/BghMLsGzlsOlT6DjGBFgVxWZU7HQrzYX8Q2beX7PeZpkUujgh7o29A42q31RzF+nzG+CmL5k44GJ2pRby0/ZU5m46jIdF0b91KLdd2JqRXaIae2+FEEIYNuBhrfVGpVQAsEEp9YvzsTe01q9WX1kp1QW4EegKNAd+VUp1cD78HjASSAb+UErN11o30Ykt5yk3i/k6Bm8PC5MGtqq5sMf1VU8HvrxrIMXldsL9PVElN8Ls23k8YQa2wpnseLMbfkNvYJ93Nzpl/sJ9HQtRM18xwc2wx5jYtyXvLNnHzN8P8sL4blWvMeFjcNjAy7/ma7cZZgKmzV9A5yuP//4OrTXBlFI1l7foB3t+NCmNvqFmWdY+WPI8XPo0hLc7/rZPl4JUM1+oy7ijH6tMD2wzDA5vgM2fQ0WZSRc9l1RWEAx3/qnxDTVzDU8kwNo5D0qyod8dENjCLDu80TTPPp0qC1xEd68aaUzZDK2HnN7XPYecvyNYYE72yQsgtDV8fgNeB5fzyrU92PjUSL66ayC3X9Saw7ml3DlrPY/O3iIjWkIIcRbQWqdqrTc6/18I7AJi6nnKOOBLrXW51joB2Af0d37t01of0FpbgS+d64rzjK+nOxEBXiilwC8cbvkOpi7mcNe78KnIwWfxP+j+wzje9nyXDklfQkWJuUnb93bC/b24qldzvt2YTH5ptQIGHt5HB1cAFnfocR3E/wzFtRodF2XULIJQmA55B2vOv6oUW6vhcFY8/O8K86F8wV9OvFKhK/34CHx9a81KdJUO/AbhHSGwObS/zBzLgyvP+C6edpl7IKCZGc2sFNbO9JVqqPXTzSho62HmuV6BJihtiPhfYNGTJ7TLR6Q7f25R3c3vQ2ALKXRxgs7vAAvAPwImf29O3C9uhH2Lcbe4MaBNGI+P6cyivw7lvkvaMntDMmPeWs76RCmCIYQQZwulVBzQG1jrXHS/UmqrUmq6UqqyNF0MUL27a7Jz2bGW136Nu5RS65VS6zMzT7xnkmiC3NygRV9aXf8K8df8yiXlr3O/9QE+7vYJ6onDcOcSuOJV8I8E4LYL4yix2nlu/g7SC8qOv/2eE83o1vZvzffZ++HryfBqe3hvAOxaYAKkyqpxlcFUdc17VzUczoqH/401RRQG3Q+JK6q2XZ3WsOI1WPvhSR4Yp+LsY1fDS99RVU583Uc1H7OVm35ObYaZ71tfDO4+sPccTBOsXkGwUlg787NuiPQdkLQa+t5uzkc3NzMfKqWBFSiXvwq/v2OadJ+otG2memCAM3ureS8p1X6CJMACE53fOt9US/liYo0eFZ7ubvztsk58ffcgAK7/cDWPz9nK4l3pFMuIlhBCNBqllD/wLfAXrXUB8D7QFugFpAKvVa5ax9N1PctrLtD6I611X61134iICJfsu2g6rujRjHsnjGJbyHAuu3QEWDyOWqdbTBCTB7Xiu82HueiVJTzyzRb2pBXWsTWnqK4Q3QM2zoIfHob3+psRh4F/MumOX91sAqYtX4LFE5r1PHobnn5mjsyeH83IlbabrJyRz5v1Fz0J5bX2Yd1HsPh5+OnvJtA5GYVp8M4F5vNSXaNky/8Nnv6mx9nWr2tWZTy0Dmyl0PYS872HD7QZagqGnEsFxrQ2I1iV868qhbY11SWtJcffxvrppp9ar5urljW/wIwKHq9ASmGaSS0F0+D6RKVtM+fWkdftZUbepNBFgzUowDreJGCl1BCl1EallE0pda3rd/MM8AuDyfMhpg/MvduUcC3NPfJw37hQfnpwCDf2b8l3m1K4Y+Z6ej2/iBs/Ws0nqxOl8qAQQpxBSikPTHD1mdZ6DoDWOl1rbddaO4D/YlIAwYxMxVZ7egsgpZ7lQtRwfb9Ylv3tEmJDfY+5znPjurH0kWHc1L8lC7amcNmby7n01d8Y+foyLntjOaPfXM6t09ex/bDzQ2rPiZC+DdbPgAsmw583weh/wr2/m0qHmbtg9wKKwrqDu1fdL9qiv+mFpR0muIrsZAK0y18z5eiX/atq3fhfYeFjpsJhSCv47l4oLzrxg7HwcSjLg32/wObPaj6WuQd2fAf974Shj5pgatOnVY8f+M2MurUaXLWs/SiTBnkmijecKfnJpiDFUSNYzkqCOQfqf355EWz5CrpeXTW/DsxnVEdF3amX1e3+AdDg4Qv7F5/YvtsrzOhbdPeqZUcKXWw9sW2dx44bYCmlLJhJwGOALsBE54Th6pKA24ATaOxwFvINhdsWwKVPwa758P5gU2XQyd/Lnf+7ujubnxnJZ1MHcPvg1uSVVPDUvB3834+7JMgSQogzQCmlgGnALq3169WWN6u22tVA5aeQ+cCNSikvpVRroD2wDvgDaK+Uaq2U8sQUwph/Jt6DODe1CvPjuXHdWP3YcB4Z1YFOzQJoF+lP63A/Wob6sjOlgKveXcmz83dQ2HWi+bxx3zoY+3pVOpbFA/rfycLhP/Gviut5IHU0fxxrekKP602xi8rgqlJsP+g1Cdb8xwQ9Gbth9hSI7ArXTIPx75t+TL88fWJvMP5X2DEHhj1ugqSFT9TsY7XiNTMqNeh+8wG95YWmSW5lafEDv5n+Zd6BVc9pP8r8ey6Va69dQbBSQyoJluXD/PtNr9Z+d9R8LOYC8+/x5mHt+t6MlnW9GvYvPbHS7ll7wW41868qNXcWujjePCx7hRmRlUCsQSNYx50ErLVO1FpvBY7fAv1s52aBIY/AHb+YPxIzr4LPrjd5rM7+A17uFga3C+fxyzvz04MXc9uFcfx3RQJv/Brf2HsvhBDng8HALcCltUqy/0sptU0ptRW4BPgrgNZ6B/A1sBNYCNznHOmyAfcDP2MKZXztXFeIUxLi58n9l7bnPzf34f1Jffjglj58dGtfFj88lJsHtGLm6kRGvLuBn0JuRleOalSz5kA2f55zgDUxt3EwqD+3z/ijauSrutj+pkR9ZM0P8lpr0vr/nQqLL6mf3k35J9eZBscTvzCFN1pdCIPug/XTzAfw6oqzTCBU+0O5tQR+eMhMp7jor3DVO+aD+PfOghrZ+2HbN2bOkF+4ec6Au8zoVPwikxWUsrFq/lWl4FgT+J0N5dqz98NHl1RVOjxZlRUEj0oRbON8nWMEWImrzM39nfNN8F299xlAYAz4R9U/D6s018zB63wltL3UjDaeSKPg6hUEK1UWujjePKxt35iAeqF0vGhImfa6JgHXUc7mHBNzAdy9HH572Qy1xjvvrHgFQqcrzIkfFINSiqfHdqHEauPtxfH4elq4Z+jRfyyFEEK4htZ6JXXPn/qxnue8BLxUx/If63ueEK4U5OPBC+O7MeGCGP4xdzv3fraRi9qF8/SVXegQFQDA3vRC7pq1nthQH6bf1o9iq53r3v+dydPX8fU9g2gbUUdlQqdNSbm88Ws8W5PzyCup4FbL1TxfMZNy7cErLd7guooQ2lSufOmTJqiZdz/86XeT1rbmfTNvyl5uKheOf78qrW35v0ywdNsPJmUxrC0Mfxp+fhy2fmU+1Fs84cIHqnao01gIaG6KajhsJpWxzbCjd7zDKHMjuywfvINccahPnLXEVD5M3w7z/wz3rTU32qvT2owu5SWZkUBnkZOjZO4Gv4ia6X1ggtuAZkcXurBZ4bd/wso3TGXrOxaZkb7alDLzsOobwdr7sznWna+CkDhAwb7FdW/PYT+6xUHaVjP3K6xWmf/mveoP1Bx2M4Jp8YKDqyBhhSliciZsnwP7foVx7x3dzqCRNGQEq0GTgBuiyVVi8vSDUS/AnzfCQ7vNL1PX8eYH+W4/WPE62Mpxc1P8c0IPruzZnJd/2s2s1YmNvedCCCGEOEv1bhnC/PsH8+yVXdh2OJ8xb63gmXnb2Z1WwOTp6/D2sDDz9v4E+3oSE+zDp1PNfe1bPl7L4bzSo7ZXXG7jue93MOH939mTVsCYbtE8P64rV93xJCU9J7Oo68vMOhTByDeW89R328kuKjfBw/j3oTAF/jMI3r8Qts2GXjc554Dthg8ugj+mmVGN398xBRfiLqp64QF3m0Dsp0dNQY4LJkNAdNXjFg8zonVgKaz7ryl+UXtUBky5doet/oIMWh+/uMPJ0hoW/NVU7rv4ERNIrnrr6PW2fGnmlCWsgI+HV6UC1lZXgYtKtUu1a23SN1e+Dr0nwd0r6g6GKsX0MVUjj1VwYtf3ZqSreW9TX6B5r7rnYWXvh1c7wKZa8+jSt0NUF9NOoLpmxyl0sWOuGZm76h0zyrbslWO/h+rKC01wdLLTbOwVsOgpMx+wsnrlWaAhAZbLJgE36UpMgc2g+7XmxLl/namAs/g580dpz0IsCl6/vicju0Tx9Lwd3DVr/VHD+Ta7gwVbU7jhw9U89/0OHI6zfM5WfnLDy4kKIYQQosHcLW7cNrj1kcIYn6w5yOg3V1BYZmPGlH60CKkqqNEmwp9Zd/SnsNzGqNeXccu0tbyzOJ7V+7NZsjudUW8sZ8aqRCYNaMWvDw3lnxN6cOugOPq2icT36re58vqp/Pa3S7ipf0s+X5fEuPdWmXLyLfrCJf8wI08jnoWHdsKVb5oiFfeuNimIPzwE00aZDJ6RL9R8E24WM2pgKwflBoMfPPqN9rnNbD9hmZm3VUcVRlr0M014f3nGfOC3Wase0xr2LIRpI+HFKJh2Gax627WfT9ZPh61fwrDHYPhT0O1acxM9J6FqndyD8OPfzLyyqb+a5sjTRtaYq39kf+sNsNrWTBFcP90EBiOfh3Hv1t07rbqY3oCuO13PWmyClU5jTVl3gLbDIXk9lObVXHf5v6EkywTHeUlV+562DaK6cZQj87DqmF/lcJjRq/CO0P06GPwXM6KZuKr+9wLw3Z/g02tMAH4yts+BgmRzfi556cTmm51GDQmwZBJwbSFxcONnMGmO+YPyxQ3w30vxiF/IuxN78eDw9qw+kM3Yd1YyZcY6ft+fxYfL9jPkX0u5//NN7M8sYsaqRJ6Zv+PsLYzhcMBn18H0y44u8yqEEEIIlwj18+SF8d348cGLubp3DP+9tS9dmx+dJte1eRBf3TWI8b1jyCgo57Vf9jLxv2u4/X/r8fZw45t7BvHC+G4EeNcRwAARAV68ML4b3957IbnFVm6dto78kgoz7/zBzWZeVfWUtqAYmDTXjGZZPODyf5sRkdrC28N1M2Hcf8xzavOPgK4TzP/bDKv7IFjc4doZ4BUA8/4Eb/eC1e+ZXl4fXGw+ZxWmmzL2FSXwy1OmVPx7A00gVJBa/0GuT/IGM2eo3QgY8qhZNupF854r5xI57DD3HvP/qz8wgenUX0263ycT4Pd3TRpe0hoTcJXnH11BsFJYOyjJNuXrM3bDz0+YIGjQA3WvX1tzZ6GLuuZh7fsVbGXQeWzVsnbDTQn/6oFg9n6TClr5c5n/gAmuCtPMvkX3OHrbzeopdLHnR1PNcsgjJrDrO8U5ivVy/e9l9w+mqJx/tEk1rSsgK0yHryY5KyPWorUZaYzoDGPfMJU3d8yt/zXPENWQD/jOycNvAhZgutb6JaXU88B6rfV8pVQ/YC4QApQBaVrrrvVts2/fvnr9+vWn/AYanc0KW74wQ7u5iSbqv+ivFEb05NMdNj5adYjcEtOMb2CbUG4f3JrhnaP418LdfLj8AFMGx/H02C6me7wrFWdDXqIZSj4Z2+eYIWsw1YKGyYRFIYTrKKU2aK3ryYM5O50z1y7R5OWVWNmYlEt2kZUrezbH28Ny/Cc5rdqXxZQZf9CjRRCf3DEAH0/zXJvdwbzNKSzZk0GfliGM7BJlStNrfWpzW9J3wDe3wc2zTYn4Y9HaBAkr34SDK82y8A5w0UMmi6hy9CsvCXb/CDu/M814lRu0G2lS7NqPAg/v4+9TWb4JiBY8ZJ5/97KaAebv78Kif8DEL01lvV+eNimVvW6qWqc0z8zbSlh29Pan/GSKidS25yf44ka47UczelSYZkrzV1aRbIi3epkiFDd8UnP5t1NNoPdIfFWKn70CXmkN3a+BK51pj/PuM+mgD24xwdGCv5oAJbAFfH7dsff99a7QciBcO61qmdbw0TBzPO9fX/W6q98zweOxtlVWYJpq+4TA5O9h+iizjbt+g6AWZp2M3eZmf36SGaH60+qqx8CcK59eY34uPW40Ka22MlOZs3aKo4s09NrVoADrdDjnLlJ2G2yfbTpnZzurCSoLjsDmZLtH4xbVmbDWvUwAFtkZ7RXA8wt2MmNVIncPacNjYzqZIEtrc1ciK96U13T3osLuYNW+LLo0DyQyoAF/NMqLzLB15m5z0lbPl24Ih93kYmtt7kztX2rubh1rMqcQQpwgCbCEaFw/bkvlvs83MqxDBO9P6sMPW1N5Z0k8idklhPh6HLk53DEqgOGdI+keE0SbCH9ahfmeUDB30pI3mIp4bS85uhBDdVn7zPybLV+Y3l/u3iYVsd1wMzIUEGVGuApTzL8Zu0zwlrbNFN3wCoRb51WVQK9krzCjZ2V5ZlSnw2Vw/SdHB5oOhxk5KS8Ca5FJ03Nzh45j6g5KM/fCe/1MGfWc/XDTN6bIx4mYfYcJDh+qVvTUZoV/t4UuV5m0zeq+vNmk9v1lq5lf9k4f6DcVxrxiPut9Mt6kEXa/Fjb8Dx5LqrvYyJc3mzla96w0o41gSvd/do2ZQnPBrVXrWkvgrZ4Q2dn0ma3th4fN/L6pv5oRwcw98N/h5nPnlJ8g+Q/TcNvdG8b8y6QSxvaDW76rOq4zrzQ//we3gLsn7FpgnjPuPRNsnwYNvXadnvDufGRxh543mtzTpDWmiVzeQdzykojITYT982DnrCOrq/COPN16CO07teHl5UV4lefw58iNuG/53AyzAqx8k9xRb3HPEgdrE3Jwd1OM7BLFxP4tuahdOG5udfziam2aB2buNkOu3041vwiVJVMbYsdc8/xrp5th4j0/mWaFV7x6aseoIazF5heu1811pyIIIYQQ4pRd3r0ZL43vzhNzt9H3xV8pKrfRpVkgH97Sh5Gdo0jKKeHXXeks3pXBh8sPYHfOG1cKWoT44OvhToXdQbnNQYXdQfNgH67vG8tVvZrj73XiHy9tdgcVdn1kNI0WDczACW8HI54xc8kSfoP4X8wozs9P1L2+xcvM+RryKMQNNv+vXS0QqtIiZ44Fv0gY+1bdAZObG0TVm7RVU0icGTHL2Q8D7jnx4ApMMLh9tkmfqxz5SlgO5QWmemBtbS8x87yy95liJcpi5kmBeU9XvWNqCmz4HwS3OnYlx3bDzXb+1daUgO98pXlOYAszglSdpy8M/jMsetKk/sVVay6dtNZ81htwd1VBj4iOJv3yq5vh0wlwaJ0paz9pNgS3hNIcM9L2x8dmjmDKJvOeR75ggiswVb6b9YLfXoHu11ctBxNceweZ/ToDZATrTNHaFI1I32E6tyetgYO/Q0UJDtywa4WHspMf1pOgQVPALxzr9w/jVpLJR45xBF72D5LybczekExOsZXYUB/uGNyaG/u3rHknadm/YemL2Ee+iL3VxXjOGAWth8BNX1dNeKyPww7/GQjKQv5ty7CjCF36d9g4ywy51tGvw6V+eAT++K8Z6r/5m7Om3KYQwrVkBEuIs8PHKw6waEc6Uy9uzcguUXVOWSgut5GQVcz+zCIOZBaTkFVMuc2Op7sFD4vC0+LGpqQ89qQX4utp4aqezZk0sBXdYo5fcl1rzYKtqTy/YCeZheV4ubsR6udJsK8nfVuF8OTYzni5Hz2CtTI+i2fmb+eVa3rQNy706A3nJZmqhOWFENjclIsPbG7mTVX/4H08mz+HyC5VRR5c4b0BJsi5c0nD0hlrS1pj5sgPfwaCYqEo3VQPTN8Bf9t39DZzEsy8tkH3m5L5fW47+qb5+ukmgOk01tQZqIvWJiVz1/emV1dBsll++asm6KnNWmzSGUuyzGfRrhOgw2iYdZUZ8btvTdVIWKUlL5mWAHEXmxRIn5Cq1/70GvP696yEJS+aFMG/7qjZuLpyRO2K16DL1bBrnpn2krgSrvnYjNKdAkkRbApsVji8Hg4sIykzj2cTurIkN4yRXaLoHxfKhz9v4Hnvz7jcvtRUZglrh8NaRF5+PvkFBawui2ORzxhGDL+M6/vG4rn/Z/jiRraFjeb2/KkUldv5uMtmBu/5p6lOU1d1n9q2fgNzpvJp7PO8kNABL3c3Pry6BYMWjDC/FNfNOH3H48BvMGuc6R6evg1GvwID7zk9r1WUaX4h3b1Oz/aFEPWSAEuIc4vWmk2H8vhyXRLfb0ml3GbnmSu7MvnCuGM+52B2MU9+t50V8Vl0jwlidLdo8ksryC22klFYzrK9mQxuF8aHt/StMSq2ZHc693y6EavNQafoABY8cBHulobUbTtL5B0yrYBq98lqKGsJvBJn+pVVsniZxs6jXqz7OW/3NtlVbh5m2kf1uUxgAphfnjKFSNqNOP4+VE5pSd1qso6OFbTmJMCmT0yQk1utKuNNX5u0y9ocDpPCGTvg6M9oBSlmECAo1mR7XfiA+Xxbe7+mjzb9vGzlpsBHWHsTWPW80dkb7ORJgNUEWW0Opq9K4O3F8ZRY7QzpEMHbN/Yi+NBiWPp/ZnTJ0xc8fNFu7jgSV2Gxl7HdEcdyzyHcZv+GffZoJtqeZXCnFljtDn7bk8Gngf9hcMVa1O0/mZKr1hLI2Y/OS6IorDvpOoS0/HJScgsZvGgsBVbFBP0K43u3ZFNSLnvTC/m6wxL6HvwY7lwKMRdgtTn4fksKi3en06NFMCM6R9I2wv/ki3WUFZh5X+7epsHz7Clm7tddS09s6L0hUrfCjDFmguit80/sTpZoPPG/mFSDiV+aRoyiSZMAS4hzV0FZBQ9/vYVfdqYzZXAcT17RBUu1aQ35pRXM/D2R95buw8PixsOjOnDroLga6wB8uyGZR7/dSrfmgcyY0p9QP08Wbk/jgS820ik6kBv7x/KPudt5YXw3bhlYT/EMJ601NofGoykFY8eSstmkBPpHmS/voPqzfiozhPrebgpanGlam7S+HXPNDe4hfzu57Wz9GubcaQLFv2wzbZRqS14PPz5igsVu15j6By7KiJIAqwlLzS9lfWIul3dvdtQfmxpK89Bbv6Zo9TQC8naTq4JZNPhLRgy8gDB/L7TW/LAtlVfn/cEs2yOEuZdhdfMhpCK9xmY2Otrxk70/AP/w+JxFXf9N/ytuI9jXkxKrjUdnb+W3rftZ7fcwXhFt+SVyCq/vCmJ/oTvh/p5kFVkJppDLgg4xOiyD9n0uocUFx5jcWY3N7uDD5QdYtDOdGSEzCY3/Bu74xeTjFmWagMs3zARZlfnRFWUm/9c/6uQ6hOcfNs0BbWVm8myfKabnR1OVlwQbPzGjk8frndGUlRfBe/2h4LDpT1K9gpFokiTAEuLcZndo/u/HXUxbmcCIzpG8dWNvMgrL+d+qBL7ZkEyJ1c6YbtE8c2VXooOOnSb368507vt8Iy1CfLhlYCte+GEXPVoE8b8p/Qn0dufGj9awJ72Q3x4ZRrDvsW+YJmYV88g3W0jOLeX7By4iIqBhGSwVdgfubsr11Z7PtKS18N09pqBHcMvG3puTVznS5htmWgucYRJgnU+0NkOhXoF13tnPL6lg5tz59NzzFsWWQHK9W1IS2Ab34Bi6VWyjffYSgvN3mU1FdUXdvbLGfC2tNR8uP8C+RR/xsvtHuCsHAMWB7fBt0RV72k7cc+JrvOZhjzjs/e6i5bApdU4o3JteyCPfbGFrcj5jvLbyvnqZ3e2m0mnSa1UrVZbf7H+XKdG6fhqsn2FyeQEufthMaq2vulB15YUwfYwpp3/7QjNBdOUbcMXr0O+Ohm3jbFJRaqpFpm2DLuNMH5KmfgE4lp//AavfNWmqexfCXctcmw9/Otkr4NdnTUWpE63oeQ6TAEuI88Os1Yk8O38H4f5eZBaV4+6muKpnDFMG2qV0twAAIABJREFUxzVojhbA2gPZTJ25nsJyG/3jQpk+pd+RlMFdqQVc8fYKbh0Ux7NXHZ3xorXm0zUH+b8fd+NhUZTbHAxqG8aM2/odN2hal5DDnz7bwMA2Ybx9Y++6i4udBjtS8vlmfTL3DG1bb/ApzjwJsMRR7A597BGxnASIX2Q+AB4jJW9FfCbLth1gUmw2caU7TIWXzD1m8mdsf4jtT55/e9b+/Bkt42fRmQSKlD/5YT3wdPfAw8MDTw93kos0a1OhxD2Qi3p0oPP+aSSXeTOi6DkmDe7AE5d3qsqlXvgErHnPlDx12M0H7P53mt4XG2eZPOFrPq6aBHnMN28zjQr3L4WbvzbPc9jhi4mwf7FJFaxe4aYhKkpNSmNjBTXz7jd5zV0nwI45ZrLrxQ81zr6cTmnb4MOhcMEtJtf6rZ6mStCt3534tsryAVVzQuzptvgFWPGqOUfvWVV3I87zkARYQpw/luxO598/72VklygmDWzZsJYztexMKeDHban86ZK2+HrWrFL45Hfb+GLdIX7888V0jK4qmnAop4Qn5m5jRXwWQzpE8K9rerBoZxpPz9vBM1d2YcrgY6ebf742iafnbSfIx4PsYiv3XdKWv13Wqd591Fqz7XA+2w8XcEWPZgT51N30uT7JuSWMf+93sorKCfLx4IXx3biqZ/MT3o44PSTAEo2qqKyCJYvm47VpGlH2VCw4cEPjhgMfrES6F+PrKDIre/hSMflH/m+TJzNWJTKwTShX9YyhebA3MQFuxK14GPfAaNSAu03JzkrrZ8CPfzMfWMf8G2ylJgWw4DAUZ5nKNL6h5oPt4Q2w7RsY+6bpMF6pLN/0XSjNgRs+M89N2WS+yguh50TTVNAnuOo56TvMyNf2b80H/VEvnnhwVslhP/YInM1qAid3L+g8rmYVyI2zTOf1IX8zo3jfTjX7c/NsaN+AyalNhcNhmg/mJMD9f5ifZ2XzwlvmmjKx9T13zw8mqM7aa3rLFaWBhy9M+rbuxod1yU00o8MnMxn5wDJTuKXjGFNONrqH6U13mhogNiUSYAkhXCW32MqwV3+jW0wg70/qw8LtaczbfJjf92fj42HhH1d05qb+LVFKobVm6sz1rIjPYt79g+ncrOYNtwq7gxcW7GTW6oMM7RDB2xN78/JPu/liXRKvXdeTa/rULA6htWZ3WiELtqawYGsqB7NLAGge5M1r1/diUNuGt5zJL63gug9+JzW/jDeu78V7v+1jU1IeV/RoxovjuhHiJ3PGG5sEWOKsYHdosovKySgsJ9P5FRnoxdAOESiH3TTws3gc6bnw7YZknpq3nRKrvcZ2PC1uBPq4E+jtQYCPB6G+HkQHedOLvVy55zF8yzOrVnb3MX2/ygvN9p3y+jyA9+jnjm6QmBVvgqzyfOfzvSkL60pxeQVheVspV94s9hjKUntP7glaQ9uc5eDpbyrSxP9igrKOl8OI5yCigxktK0ozwZ52mC701Xt6FWdXlQ09uApaXmga4nUZZ9IpbVbY8jksf810LwcTyF32khlhTN0CH4+EVoNg0hwToFlLYNoos/6dS09/Of36lOaaADR5Awx5uP4g6HjWz4AFf4HxH0CviWaZrRze6Qu+IXDnb0e3H9DapBEufcmMfnkHmSqc4R1Mv5RNn5mStrf9AM16HPu1tYYNM+Cnv5seKLd+ZxogNlRxFrw/2IyW3fWbaYA49y4Y+hhc8nj9z7XbTPnb4FbnbNqnBFhCCFea+Xsiz8zfgYdFUWHXtAz1ZXyv5lzfL5YWITWnKmQXlTP6rRUE+3jw/QMX4e1hodxmZ8XeLD5afoB1iTncNaQNfx/dCYubosLu4LYZ61iXkMNnUwfSv3UoDodm0c50Pli2n82H8rC4KS5sG8bYHs1oEeLLk99tJzG7mKkXtebhUR2P25zZanMw5X/rWHsgh1m39+fCduFH5qq/+etegnw8uWlAS8b2aEaHqIB6t9VYMgrLuHPWBu4d2obR3eooPnEOkABLNFk2u4OMwnJS8kpJyS8jJa+UvJIKCsoqKCitIL+0gpxiK+kFZWQVWQmhgJ5u+yn3jqRL5y5c2qsjvVuFsOZANr9sT2HdrgSKiotJx4xAhPt70izIh7hwP9pH+tM+0p9u6gD21G0syW/OVwf92JNZBkBPSyJ3+y5huG0FXrqcXO3P15YriLj0AcZd2A2LvQzW/AdWvAEVJaa/RmGqKQtanW+Y+ZDv7ml6MThsENbOVLjZt9iULvUKNE3yEleZQCmmLwx7DEpyYPFzJpDrNNYEDQ6bqbZYvYF0biJ8NMzsw9C/myDQ068qaCvLNwFnWb4J/Dz9zSifl78Z5QtpfWqpcxVlpkLR8lfNa/hHmUCz1yS47MWaaZyFaWaOnd1qAiDvIPAOdqZcupkvW2lV2f7bFtQMNLZ8CXPvhmumVfW0sJaYnicrXzcjliGtzfHrfl3NUcL8ZJh2mSlve/vPdQej1hL44SHY8oXp3ZGxywRct8yBZj1rrpuXZDrOtx5WFUhrDZ/fYFoP3LnYVKwE04l+yxcmJfVYRVqSN8CCB83POe5iGPUCNO99Ij+JJkECLCGEK9nsDh6dvRV/b3fG946hd2xwvXOslu/N5Nbp67iiRzN8PCz8vCONwjIbwb4ePD22CxMuqDlSlV9SwdXvryK32Mp9l7Tj87VJHMgqJjbUh9sHt+bKns0J968qnFFitfHPH3fzyZqDdIwK4PUbetK1ed1zzrTWPDp7K99sSObV63pyba1Rsp0pBbz0405W78/GoaF9pD9X9GjGpIGtarxmY6ocGVy8O4MwP0+WPDyMIN8TT5E820mAJc4LVpuDzKJythzK44dtqSzZlUFpRVVw4+/lziWdIrmkYwQODSl5paTml3I4r4yErCIO5ZTW2J67m2JgmzBGdI5kSIcIWob6mvlgpbmQtJZNlm68sOggG5Py6BgVwGXdomkX6U8H/zLa7ZuBe0kmBMaYtMXAFqAUtow9FB7ajiNjN5TlkxI5hIxWYyGqO8F+XnSO9sc3da2pBrhznpkDN+xx0zG98uJgLTGpcSvfMEHJlJ8gtt/RB2T/UvjsOnBUnNwB9YswaZghceAbboIi3xAT/Fg8TOCANv/aypxBWz6U5pnqjvmHzPy2Ec+avhPLXoFVb5lAcMSzJvjc/YMJgBrCzQPuXWU6vFfncMCHQ8BaCP3uNMHawd9N0BQUC0MfNemdlmP8cc+KN00aPfzgjp9N88lK2fvh61tNKujQv5tt5SSYYK+80MzhaznQLFv5umlC6bCZeYJth5uAriDZFLYY82/Tl6RSeZEJgq1FplFi9QC5NA8WP2+aPQZEm34dG2dBSbbpSH/pkxBSRxni8kLY8R1s/cqcpzEXmOC8RV8TbKdvN6OeqVtM75V2I8xoYPUeKEUZsGEmbJwJFk/ofCV0vsps6zSNoEmAJYRobC8u2MnHKxMI8HJnVNdoxvZsxkXtwo9Zxj0xq5jx/1lFXkkFXZsHcs/QtozpFl1vD66lezJ4dPZW8kqsPDi8PfcMbVtj/bT8Ml7/ZQ9fr0/mz5e246FRHY+5rYzCMn7ensaCramsS8yheZAPM6b0OytGtL5cl8Rjc7YxsX9LvvojiUkDW/H8uG6NvVsuJwGWOC+VWu38tieDbYfzGdAmjIFtQuvsAF+pxGrjQGYxe9ML8XK3cHGHcAK967/jorXmp+1pvL04nr3phTicv0JKQbCPB76e7vh7uePnZaHEamdfRhE2x7F/zyxuim7NA+nTKpS+rYIJ9PGk3GbHanNQbnMQF+5HzxZB5k5cUQYUZ9ZZiMTu0KxLyGHdjnjcSzLw0qV42kvx0qU0Dw2kd8fW+AeFOXtlWMBaiC4rICElg+LcVLp4ZWPJSzCBQ26iGTmrKG7QccczACI7w6X/MKNy1aVshvn3mxEZgOYXQKfLTVqlT2hVkFaWb4I27aj6iugE0cf4A13ZrR3M6GC7ESYojbu4Yb3NUjbB/640o06hbU3aYGGaCWh8gmHCf6H9yKr18w7BJ+NN6mfH0aaLvZs79JkMXcabIjHbZld1tu94Odz4+dEBSto2k5Kq3MA/0gS1fhEm6CzJgv53wyVPmNHEsnxY+aYZJXXYzTEObW2C4KBYOLTW7Iet1LyHkDiznWqpsUcEtgD/CPO+Uebn1PVqk6a6Y64J3NtcYvY3YbkJGgNbmAI27l7mvVo8TNA7/OlTbgsgAZYQorHZHZrNh3Lp2jzouCl8lfZlFJFVVM6A1qENLt2eW2zlqXnbWbA1lV6xwbx2fU/8PN35YNl+Pl+XhMOhmTI4jicu79zgbW5LzueOmX9QarXz/qQ+XNS+6oad1pr1B3M5mF3CoLZhxAT7NGibJyspu4Qxby2nZ2wwn94xgOcX7GTW6kTm33/RUZUis4rK2X44nxBfT0L9zJevp6XJlMGXAEuIM6Cswk5idjH7MorYl1FEdpGVYquN4nIbJVY7nhY3OjULoGN0IJ2jA4gN9aW43EZBmY380goyC8vZfCiXPxJz2XIoj3Kbo87X6RQdwMT+LRnfO+ZIVSKHQ5NVVM6utEIWbk/jl51pZBVZ8bS44eNpQWuNxlxAKvdlaMcIruzZnABvd37dmc7iXRmkFZh0yJahvtw7rC0TLoipCkpt5WZkpTS3WtqjotTmwM3DBy//EJPaeJyiDfYKK3k7FxMa1wPlyip6B383IzEn29MjYQUsfMyM2vhHQUAUBDQ3Izx1bbMoEz6dYIpm9Jli+o9Vb3LocJigJ3EF9Jt67MIYCcthz0ITUBVnmi+fEFMlsa50wPxkWPsBZOyGnAOQd9AEQN5Bpoliz5vMiJVSZnQxez8cXm8CxuhuEN3TBFdgAugtX5iRt/xDJjjudZOpzlk5x6w01+zfrvmQudvMCXNUmJLzjgp4cMvxK3cehwRYQojzzfdbUnhq3nZKrXY05jp+bZ8W3HdJO2JDj25pczyH80q5fcYf7M8s4v+u7s6Y7tHM3XSYz9YksSe98Mh67SP9Gdohgks7RzKoTZhLgxm7Q3PjR6vZnVrIwr8OISbYh/zSCoa/towWIT7MuffCI+Xtf9+fxf2fbyKn2FpjG9GB3nx0ax96tAiu6yXOKhJgCdHEWG0OdqUWUG5z4OXuhpeHGx4WN9Yl5PDFuiS2Jufj7eFGj5hgMgrLSMkrw2o3AZmvp4VLO0VyefdmDOsYUaOErdaaLcn5fL8lhQVbU0gvKD/ynCHtIxjRJQo/TwsfLNvPluR8ogO9mXpxawa2CaNdpP+Ru3plFXaW7s5g3uYUluzJwE3BgNZhXNw+nKEdImgb4U+x1QSPBaUVpOaXsikpj41JuWxOyqPYaqdLs0DuHtqGK7o3O2ZKhdXmYNW+LH7ankpybimdogPp2jyQrjGBtI3wP2bqxhljKzcl+n0a8UJgt0Fhiim+4XGSPVIcDtM/L6ytmYt3hkmAJYQ4H2UUlPGvn/fgYXHj3qFtaRl24oFVdQVlFdz32UZWxGfh7eFGWYWD7jFBTBrYku4xwfy+P4tlezNZeyAHq93BpZ0ieWF8N5eNan2wbD8v/7T7qAqLczYm89DXW3h5Qndu6BfLtJUJ/POn3bQO9+OpsV2osDnIKbaSXWzl0zUHKSitYMaUfvSNO/GKvVprym2O445Caq1PObiUAEuIc8z2w/l8vi6JPWmFNAvyJibEh5hgH1qF+TGgdWiD0hscDpM2UFZhp3+t52itWRGfxbtL97EuIQcwAyKtQn2JDfVlc1IeheU2wv29GNvDjNosj8/kQGbxkXVr/zmxuCk6RQfQp1UILUJ8+Hp9MvsyiogJ9uGOi1rTLtKfEqud0gobReV2Nh7M5ddd6RSW2fD3cqd1uB/xGYWUVZhA0tvDjb6tQrmwXRiD24bTLSYIq81BYnYxiVnFHMgqJqOgjKxiK9lF5eQUW7E7NCG+noT4eRLi60G4vxdx4X60CfcjLtyPMD/Psyo1QWuT6rn6QDajukTTpfkZ7Nl1Bp1KgKWUigVmAdGAA/hIa/2WUioU+AqIAxKB67XWucr8gN8CLgdKgNu01hud25oMPOnc9Ita65n1vbZcu4QQZ5sKu4N/LdxNYZmNif1b0jP26BuAJVYbn69N4rVFe3FT8OjoTkwa2OrY/VGPo9xm58t1h3jph11c2imS9yddUONaqrXmhg/XEJ9RyOB24SzYmsrortG8en3PI02iK6XklXLzx2tJyy9j2uS+XNguvPbLHVNCVjGPzt7CH4m5BHi5ExXkTXSgNyF+nhSUVpBbYjVfxRX8c0J3rjzFnmISYAkhTtqBzCJ2pxWyN918HcgspntMEON6xTCwTWiN0adDOSWsiM8iJa+UQB93gnw8CPT2IMzfi24xgTVG0xwOzZLdGXy4fD9/JOYe9bpBPh6M7BLFmG7RXNQ+HC93Cza7g4SsYnakFLD5UB6/789ib7rpoVZ5t666QG93wv29CPM3ud0WN0VucdUf2ewia405cX6eFvy93fH1dMfHw4K3hxsObUbSKuwOrHYHHhY3ArydbQK83WkZ6svwzlH0jg0+kvpQF5vdQWGZjcIyG5GBXvUGwXaH5ucdaXy4/ABbDlXNoerbKoRbBrViTLdm2B2anan5bDmUz/bD+eSVVmBzaOwOBza7xu7Qzu/NV4C3O2O6RTO2VnWr6j+PtIIyErOKScgu5mB2CV7ubrSL9KddpD9tI/zxtLiRVlBGUk4JSdklHMot4a8jOtT7vhviFAOsZkAzrfVGpVQAsAEYD9wG5GitX1ZKPQaEaK3/rpS6HHgAE2ANAN7SWg9wBmTrgb6Adm6nj9b66JPTSa5dQoimrHrz5Z6xwfSuFYx1iApgRJfIYzaDrrA7mLMxmbcX7+NwXin9W4fywaQ+hNbRo2t3WgFXvL0Sh9Y8MqojfxrW9pg3NDMKy5j08VoSs0v4cFIfOkQHsCetgN1phezPKKZdpD9jezQ7kkrpcGhmrU7k5YW78bS4ccugVhSX20nLLyOtoIy8EitBPh4EO+d6Bft6cHXvmFNOQ5QASwhxVtuVWkBRuQ1fTwu+nu74eloI8/OstxpTpczCcn7fn8WmpDxC/TxpHe5Ha+eIVO07Y7XZ7A6Sc0tJyC4mIbOYpJwSSqw2SisclFrtlFXYcXNTeFoUnu4mTbPC7qCg1EZhWQUFZTYO5ZRgc2jC/T0Z3imKPq1CSK8MQnJKSM4tJb+0gqJy25HX9bAoOjcLpFdsML1bBhPo7UFGYTkZBeVkFJaxcl8WB7NLiAvzZerFbRjROYoFW1P4dM1BErNLCPB2p8Rqx+4MDqMCvYgM8MbipnB3U1iqfVV+n5xbyu60QixuisHtwhneKZLsYisHMovYn1lMQlZRjQDV090Nm91Ro3CLh5vbkVRUMKOSqx+7lMjAk0xNpHLbrksRVErNA951fg3TWqc6g7DftNYdlVIfOv//hXP9PcCwyi+t9d3O5TXWq4tcu4QQTZ3Wmu82H+a1RXspLKu6TtkdmqJyG0pBr9hgRnWJpm2EH1lFVtPLtKiMlfFZJGaX0DM2mEdGdeCiduH1ZoEs2pFGoI8HA9scv+FyTrGVW6evZfvhghrLw/29yCoy0xt6xQZzRfdmLN6dzpoDOQzrGMHLE3oQHXRq16SGkgBLCCFOk/zSCn7bk8EvO9NZtieTQmcgFRngRctQX1qE+BDq53VkRM/P052E7GI2JeWyNTn/qEbaIb4etI8K4PbBcYzsEl0jZcPh0KzYl8UPW1OICvSmR4tgerQIIqqBAc6etELmbT7MvM0pHM4rRSmIDfGlTYQfbcL9aRNRFZw2C/SmwmFGDPdlFBGfXkRZhZ3YUF9ahfnSKtSP5sHeDQqCj8dVAZZSKg5YDnQDkrTWwdUey9VahyilFgAva61XOpcvBv6OCbC8tdYvOpc/BZRqrV891uvJtUsIca7SWrM3vYhFO9JYtDOdbYfzazwe4utBmwh/7hnalhGdI09Len1+aQWfrE4kyNeTTtEBdIgKIMjHg0M5JSzYmsqCrSnsSCnA38udp8Z25vq+sWc0zV8CLCGEOAOsNgfJuSU0C/LBx/P48+DsDk18RiElVjtRgd6E+3vW20rAVbTWJOeWEhFQf6rimeKKAEsp5Q8sA17SWs9RSuUdI8D6AfhnrQDrUeBSwKtWgFWitX6t1uvcBdwF0LJlyz4HDx48ld0WQogmITW/lMzCciICvAjz88LTvZGLTDkdyinBz8u9zrTE062h1676c2mEEELUy9PdjTYRDe8JZQp/nPnCFUqpkyoDfLZSSnkA3wKfaa3nOBenK6WaVUsRzHAuTwZiqz29BZDiXD6s1vLfar+W1voj4CMwNwdd+DaEEOKs1SzIh2ZBp7eH1sloCteysyMUFUIIIRrIWRVwGrBLa/16tYfmA5Od/58MzKu2/FZlDATytdapwM/AKKVUiFIqBBjlXCaEEEKcNBnBEkII0dQMBm4BtimlNjuXPQG8DHytlLoDSAKucz72I6aC4D5MmfYpAFrrHKXUC8AfzvWe11rnnJm3IIQQ4lwlAZYQQogmxTmX6lizmofXsb4G7jvGtqYD0123d0IIIc53kiIohBBCCCGEEC4iAZYQQgghhBBCuIgEWEIIIYQQQgjhIhJgCSGEEEIIIYSLSIAlhBBCCCGEEC4iAZYQQgghhBBCuIgy1Wsb4YWVygQOnuDTwoGs07A75zM5pq4lx9O15Hi61tl0PFtprSMaeydO1Eleu+DsOvbnAjmeriXH07XkeLre2XJMG3TtarQA62QopdZrrfs29n6cS+SYupYcT9eS4+lacjwbjxx715Lj6VpyPF1LjqfrNbVjKimCQgghhBBCCOEiEmAJIYQQQgghhIs0tQDro8begXOQHFPXkuPpWnI8XUuOZ+ORY+9acjxdS46na8nxdL0mdUyb1BwsIYQQQgghhDibNbURLCGEEEIIIYQ4azWZAEspNVoptUcptU8p9Vhj709To5SKVUotVUrtUkrtUEo96FweqpT6RSkV7/w3pLH3tSlRSlmUUpuUUguc37dWSq11Hs+vlFKejb2PTYlSKlgpNVsptdt5rg6Sc/TkKaX+6vx9366U+kIp5S3n6Jkl165TI9eu00OuXa4l1y7XOheuXU0iwFJKWYD3gDFAF2CiUqpL4+5Vk2MDHtZadwYGAvc5j+FjwGKtdXtgsfN70XAPAruqff8K8IbzeOYCdzTKXjVdbwELtdadgJ6YYyvn6ElQSsUAfwb6aq27ARbgRuQcPWPk2uUScu06PeTa5Vpy7XKRc+Xa1SQCLKA/sE9rfUBrbQW+BMY18j41KVrrVK31Ruf/CzG//DGY4zjTudpMYHzj7GHTo5RqAVwBfOz8XgGXArOdq8jxPAFKqUBgCDANQGtt1VrnIefoqXAHfJRS7oAvkIqco2eSXLtOkVy7XE+uXa4l167Toslfu5pKgBUDHKr2fbJzmTgJSqk4oDewFojSWqeCuZABkY23Z03Om8CjgMP5fRiQp7W2Ob+X8/TEtAEygRnO1JWPlVJ+yDl6UrTWh4FXgSTMxSkf2ICco2eSXLtcSK5dLiPXLteSa5cLnSvXrqYSYKk6lkn5w5OglPIHvgX+orUuaOz9aaqUUmOBDK31huqL61hVztOGcwcuAN7XWvcGipGUipPmzPcfB7QGmgN+mFS12uQcPX3kb4KLyLXLNeTadVrItcuFzpVrV1MJsJKB2GrftwBSGmlfmiyllAfmAvWZ1nqOc3G6UqqZ8/FmQEZj7V8TMxi4SimViEn7uRRzVzDYOaQNcp6eqGQgWWu91vn9bMxFS87RkzMCSNBaZ2qtK4A5wIXIOXomybXLBeTa5VJy7XI9uXa51jlx7WoqAdYfQHtnBRFPzGS3+Y28T02KM8d6GrBLa/16tYfmA5Od/58MzDvT+9YUaa0f11q30FrHYc7HJVrrm4GlwLXO1eR4ngCtdRpwSCnV0bloOLATOUdPVhIwUCnl6/z9rzyeco6eOXLtOkVy7XItuXa5nly7XO6cuHY1mUbDSqnLMXdZLMB0rfVLjbxLTYpS6iJgBbCNqrzrJzC57F8DLTEn9XVa65xG2ckmSik1DHhEaz1WKdUGc1cwFNgETNJalzfm/jUlSqlemInXnsABYArmRpCcoydBKfUccAOmEtsmYComb13O0TNErl2nRq5dp49cu1xHrl2udS5cu5pMgCWEEEIIIYQQZ7umkiIohBBCCCGEEGc9CbCEEEIIIYQQwkUkwBJCCCGEEEIIF5EASwghhBBCCCFcRAIsIYQQQgghhHARCbCEOEsppYYppRY09n4IIYQQDSXXLiEkwBJCCCGEEEIIl5EAS4hTpJSapJRap5TarJT6UCllUUoVKaVeU0ptVEotVkpFONftpZRao5TaqpSaq5QKcS5vp5T6VSm1xfmcts7N+yulZiuldiulPnN2NRdCCCFOiVy7hDh9JMAS4hQopTpjuo0P1lr3AuzAzYAfsFFrfQGwDHjG+ZRZwN+11j2AbdWWfwa8p7XuCVwIpDqX9wb+AnQB2gCDT/ubEkIIcU6Ta5cQp5d7Y++AEE3ccKAP8IfzBp0PkAE4gK+c63wKzFFKBQHBWutlzuUzgW+UUgFAjNZ6LoDWugzAub11Wutk5/ebgThg5el/W0IIIc5hcu0S4jSSAEuIU6OAmVrrx2ssVOqpWuvp42zjWMqr/d+O/M4KIYQ4dXLtEuI0khRBIU7NYuBapVQkgFIqVCnVCvO7da1znZuAlVrrfCBXKXWxc/ktwDKtdQGQrJQa79yGl1LK94y+CyGEEOcTuXYJcRrJHQUhToHWeqdS6klgkVLKDagA7gOKga5KqQ1APibXHWAy8IHzInQAmOJcfgvwoVLqeec2rjuDb0MIIcR5RK5dQpxeSuv6Rn+FECdDKVWktfZv7P0QQgghGkquXUK4hqQICiGEEEIIIYSLyAiWEELJXbwoAAAAUElEQVQIIYQQQriIjGAJIYQQQgghhItIgCWEEEIIIYQQLiIBlhBCCCGEEEK4iARYQgghhBBCCOEiEmAJIYQQQgghhItIgCWEEEIIIYQQLvL/N4bafiALy9cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "training   (min:    0.045, max:   32.773, cur:    0.048)\n",
      "validation (min:    0.057, max:    0.764, cur:    0.058)\n",
      "\n",
      "MAE:\n",
      "training   (min: 2004.621, max: 10312.761, cur: 2051.500)\n",
      "validation (min: 2234.278, max: 5970.878, cur: 2278.364)\n",
      "\n",
      "Early Stopping. Loading best model.\n"
     ]
    }
   ],
   "source": [
    "patience_countdown = patience\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    logs = {}\n",
    "    \n",
    "    print(\"Training:\")\n",
    "    train_loss = train_mae = train_batches = 0    \n",
    "    model.train(True)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch):\n",
    "        title_ix = torch.tensor(batch[\"Title\"], dtype=torch.int64).cuda()\n",
    "        desc_ix = torch.tensor(batch[\"FullDescription\"], dtype=torch.int64).cuda()\n",
    "        cat_features = torch.tensor(batch[\"Categorical\"], dtype=torch.float32).cuda()\n",
    "        reference = torch.tensor(batch[target_column], dtype=torch.float32).cuda()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "        loss = compute_loss(reference, prediction)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.data.cpu().numpy()\n",
    "        train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        train_batches += 1\n",
    "    \n",
    "    logs['loss'] = train_loss.item() / train_batches\n",
    "    logs['MAE'] = train_mae.item() / train_batches\n",
    "#     print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "#     print(\"\\tMAE:\\t%.5f\" % (train_mae / train_batches))\n",
    "#     print('\\n\\n')\n",
    "    \n",
    "    print(\"Validation:\")\n",
    "    val_loss = val_mae = val_batches = 0\n",
    "    model.train(False)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "        title_ix = torch.tensor(batch[\"Title\"], dtype=torch.int64).cuda()\n",
    "        desc_ix = torch.tensor(batch[\"FullDescription\"], dtype=torch.int64).cuda()\n",
    "        cat_features = torch.tensor(batch[\"Categorical\"], dtype=torch.float32).cuda()\n",
    "        reference = torch.tensor(batch[target_column], dtype=torch.float32).cuda()\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "        loss = compute_loss(reference, prediction)\n",
    "\n",
    "        val_loss += loss.data.cpu().numpy()\n",
    "        val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        val_batches += 1\n",
    "        \n",
    "    logs['val_loss'] = val_loss.item() / val_batches\n",
    "    logs['val_MAE'] = val_mae.item() / val_batches\n",
    "#     print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "#     print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "#     print('\\n\\n')\n",
    "\n",
    "    # Live Loss Visualization\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    print()\n",
    "    \n",
    "    # Maintain the best-on-validation snapshot\n",
    "    if best_val_mae is None:\n",
    "        best_val_mae = logs['val_MAE']\n",
    "        best_model_dict = deepcopy(model.state_dict())\n",
    "        # Removing the first log because first loss is usually too big and it spoils the plot.\n",
    "        liveloss.logs.pop(0)\n",
    "    elif logs['val_MAE'] < best_val_mae:\n",
    "        best_val_mae = logs['val_MAE']\n",
    "        best_model_dict = deepcopy(model.state_dict())\n",
    "        patience_countdown = patience\n",
    "    else:\n",
    "        patience_countdown -= 1\n",
    "        if patience_countdown <= 0:\n",
    "            print(\"Early Stopping. Loading best model.\")\n",
    "            model.load_state_dict(best_model_dict)\n",
    "            break\n",
    "\n",
    "    print(\"Patience =\", patience_countdown)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5ca140ba424d7d8aa154a8e4e1cb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=765), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.05748\n",
      "\tMAE:\t2234.27799\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final eval:\")\n",
    "val_loss = val_mae = val_batches = 0\n",
    "\n",
    "for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "    title_ix = torch.tensor(batch[\"Title\"], dtype=torch.int64).cuda()\n",
    "    desc_ix = torch.tensor(batch[\"FullDescription\"], dtype=torch.int64).cuda()\n",
    "    cat_features = torch.tensor(batch[\"Categorical\"], dtype=torch.float32).cuda()\n",
    "    reference = torch.tensor(batch[target_column], dtype=torch.float32).cuda()\n",
    "\n",
    "    prediction = model(title_ix, desc_ix, cat_features)\n",
    "    loss = compute_loss(reference, prediction)\n",
    "\n",
    "    val_loss += loss.data.cpu().numpy()\n",
    "    val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "    val_batches += 1\n",
    "\n",
    "print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: explaining network predictions\n",
    "\n",
    "It's usually a good idea to understand what your model does before you let it make actual decisions. It's simple for linear models: just see which words learned positive or negative weights. However, its much harder for neural networks that learn complex nonlinear dependencies.\n",
    "\n",
    "There are, however, some ways to look inside the black box:\n",
    "* Seeing how model responds to input perturbations\n",
    "* Finding inputs that maximize/minimize activation of some chosen neurons (_read more [on distill.pub](https://distill.pub/2018/building-blocks/)_)\n",
    "* Building local linear approximations to your neural network: [article](https://arxiv.org/abs/1602.04938), [eli5 library](https://github.com/TeamHG-Memex/eli5/tree/master/eli5/formatters)\n",
    "\n",
    "Today we gonna try the first method just because it's the simplest one.\n",
    "\n",
    "__Your task__ is to measure how does model prediction change if you replace certain tokens with UNKs. The core idea is that if dropping a word from text causes model to predict lower log-salary, than this word probably has positive contribution to salary (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(model, sample, col_name='Title'):\n",
    "    \"\"\" Computes the effect each word had on model predictions \"\"\"\n",
    "    \n",
    "    # compute model prediction on sample (scalar float log-salary)\n",
    "    # <YOUR CODE>\n",
    "    model.eval()\n",
    "    \n",
    "    # Store tokens for output\n",
    "    tokens = sample[col_name].split()\n",
    "    \n",
    "    # Represent as pandas DataFrame\n",
    "    sample = sample.to_frame('original').transpose()\n",
    "    \n",
    "    batch = generate_batch(sample, max_len=max_len)\n",
    "\n",
    "    title_ix = torch.tensor(batch[\"Title\"], dtype=torch.int64).cuda()\n",
    "    desc_ix = torch.tensor(batch[\"FullDescription\"], dtype=torch.int64).cuda()\n",
    "    cat_features = torch.tensor(batch[\"Categorical\"], dtype=torch.float32).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        baseline_pred = float(model(title_ix, desc_ix, cat_features))\n",
    "    \n",
    "    # for each i-th token in :col_name:, compute predictions on a copy of data\n",
    "    # where i-th token is dropped (UNK)\n",
    "    # <YOUR CODE>\n",
    "    def dropped(title, desc):\n",
    "        \"\"\"A generator for (title, desc) pairs with a value in col_name column dropped. \"\"\"\n",
    "        if col_name == 'Title':\n",
    "            for i in range(title.shape[1]):\n",
    "                title_copy = title.clone()\n",
    "                title_copy[0,i] = token_to_id[UNK]\n",
    "                yield title_copy, desc\n",
    "        elif col_name == 'FullDescription':\n",
    "            for i in range(desc.shape[1]):\n",
    "                desc_copy = desc.clone()\n",
    "                desc_copy[0,i] = token_to_id[UNK]\n",
    "                yield title, desc_copy\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions_without_word = [\n",
    "            float(model(title, desc, cat_features)) for title, desc in dropped(title_ix, desc_ix)\n",
    "        ]\n",
    "    \n",
    "    score_differences = [\n",
    "        prediction - baseline_pred for prediction in predictions_without_word\n",
    "    ]\n",
    "    \n",
    "    # return a list of pairs: [(token, score_difference)]\n",
    "    return list(zip(tokens, score_differences)) # <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Id                                                             71336034\n",
      "Title                                                      test manager\n",
      "FullDescription       test manager non functional oat an experienced...\n",
      "LocationRaw                                 Lancashire Manchester M21 0\n",
      "LocationNormalized                              Manchester Science Park\n",
      "ContractType                                                        NaN\n",
      "ContractTime                                                  permanent\n",
      "Company                                                           Other\n",
      "Category                                                        IT Jobs\n",
      "SalaryRaw                                           60000.00 GBP Annual\n",
      "SalaryNormalized                                                  60000\n",
      "SourceName                                                 jobserve.com\n",
      "Log1pSalary                                                     11.0021\n",
      "Name: 168956, dtype: object\n",
      "[('test', -0.040266990661621094), ('manager', -0.38754844665527344)]\n"
     ]
    }
   ],
   "source": [
    "# debugging area\n",
    "sample = data.loc[np.random.randint(len(data))]\n",
    "print(\"Input:\", sample)\n",
    "\n",
    "tokens_and_weights = explain(model, sample, \"Title\")\n",
    "print(tokens_and_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display_html\n",
    "\n",
    "def draw_html(tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
    "              token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
    "              font_style=\"font-size:14px;\"\n",
    "             ):\n",
    "    \n",
    "    def get_color_hex(weight):\n",
    "        rgba = cmap(1. / (1 + np.exp(weight)), bytes=True)\n",
    "        return '#%02X%02X%02X' % rgba[:3]\n",
    "    \n",
    "    tokens_html = [\n",
    "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
    "        for token, weight in tokens_and_weights\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
    "    if display:\n",
    "        display_html(HTML(raw_html))\n",
    "        \n",
    "    return raw_html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 122797\n",
      "Salary (gbp): 8.565683364868164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:20px;\"><span style=\"background-color: #6969FF\">assistant</span> <span style=\"background-color: #DADAFF\">manager</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"font-size:14px;\"><span style=\"background-color: #E2E2FF\">to</span> <span style=\"background-color: #AEAEFF\">assist</span> <span style=\"background-color: #BABAFF\">the</span> <span style=\"background-color: #9292FF\">shop</span> <span style=\"background-color: #FFFEFE\">manager</span> <span style=\"background-color: #CECEFF\">in</span> <span style=\"background-color: #FFFEFE\">providing</span> <span style=\"background-color: #FFC6C6\">a</span> <span style=\"background-color: #FFF0F0\">high</span> <span style=\"background-color: #FFACAC\">quality</span> <span style=\"background-color: #C8C8FF\">retail</span> <span style=\"background-color: #D2D2FF\">service</span> <span style=\"background-color: #CACAFF\">in</span> <span style=\"background-color: #ECECFF\">relation</span> <span style=\"background-color: #FEFEFF\">to</span> <span style=\"background-color: #9E9EFF\">agreed</span> <span style=\"background-color: #7C7CFF\">performance</span> <span style=\"background-color: #C3C3FF\">targets</span> <span style=\"background-color: #FFFCFC\">.</span> <span style=\"background-color: #D2D2FF\">to</span> <span style=\"background-color: #9A9AFF\">deputise</span> <span style=\"background-color: #B8B8FF\">for</span> <span style=\"background-color: #DEDEFF\">the</span> <span style=\"background-color: #D8D8FF\">manager</span> <span style=\"background-color: #E0E0FF\">in</span> <span style=\"background-color: #F0F0FF\">his</span> <span style=\"background-color: #C0C0FF\">or</span> <span style=\"background-color: #C2C2FF\">her</span> <span style=\"background-color: #CECEFF\">absence</span> <span style=\"background-color: #FFEAEA\">,</span> <span style=\"background-color: #E2E2FF\">undertaking</span> <span style=\"background-color: #E8E8FF\">all</span> <span style=\"background-color: #FFECEC\">appropriate</span> <span style=\"background-color: #FF7070\">duties</span> <span style=\"background-color: #FF7676\">to</span> <span style=\"background-color: #FEFEFF\">ensure</span> <span style=\"background-color: #8383FF\">the</span> <span style=\"background-color: #CECEFF\">continued</span> <span style=\"background-color: #DCDCFF\">operation</span> <span style=\"background-color: #FFE0E0\">of</span> <span style=\"background-color: #8282FF\">the</span> <span style=\"background-color: #D8D8FF\">shop</span> <span style=\"background-color: #D6D6FF\">.</span> <span style=\"background-color: #FCFCFF\">maximise</span> <span style=\"background-color: #8282FF\">shop</span> <span style=\"background-color: #FFE2E2\">income</span> <span style=\"background-color: #F0F0FF\">and</span> <span style=\"background-color: #B2B2FF\">achieve</span> <span style=\"background-color: #B8B8FF\">agreed</span> <span style=\"background-color: #8383FF\">performance</span> <span style=\"background-color: #C3C3FF\">targets</span> <span style=\"background-color: #E8E8FF\">.</span> <span style=\"background-color: #CACAFF\">maintain</span> <span style=\"background-color: #B2B2FF\">appearance</span> <span style=\"background-color: #BCBCFF\">of</span> <span style=\"background-color: #9898FF\">shop</span> <span style=\"background-color: #5C5CFF\">premises</span> <span style=\"background-color: #FFD6D6\">for</span> <span style=\"background-color: #FFE0E0\">trading</span> <span style=\"background-color: #9393FF\">hours</span> <span style=\"background-color: #A0A0FF\">,</span> <span style=\"background-color: #BCBCFF\">ensuring</span> <span style=\"background-color: #DEDEFF\">they</span> <span style=\"background-color: #A8A8FF\">are</span> <span style=\"background-color: #AAAAFF\">clean</span> <span style=\"background-color: #B3B3FF\">and</span> <span style=\"background-color: #9696FF\">tidy</span> <span style=\"background-color: #D2D2FF\">at</span> <span style=\"background-color: #C2C2FF\">all</span> <span style=\"background-color: #CCCCFF\">times</span> <span style=\"background-color: #CECEFF\">and</span> <span style=\"background-color: #C8C8FF\">that</span> <span style=\"background-color: #D8D8FF\">goods</span> <span style=\"background-color: #D6D6FF\">are</span> <span style=\"background-color: #DCDCFF\">displayed</span> <span style=\"background-color: #A0A0FF\">in</span> <span style=\"background-color: #B0B0FF\">an</span> <span style=\"background-color: #EAEAFF\">attractive</span> <span style=\"background-color: #EAEAFF\">and</span> <span style=\"background-color: #BEBEFF\">presentable</span> <span style=\"background-color: #AEAEFF\">manner</span> <span style=\"background-color: #AEAEFF\">.</span> <span style=\"background-color: #CACAFF\">with</span> <span style=\"background-color: #ECECFF\">the</span> <span style=\"background-color: #E3E3FF\">shop</span> <span style=\"background-color: #FFF8F8\">manager</span> <span style=\"background-color: #EAEAFF\">,</span> <span style=\"background-color: #B6B6FF\">accept</span> <span style=\"background-color: #FFB0B0\">,</span> <span style=\"background-color: #9898FF\">sort</span> <span style=\"background-color: #FFFAFA\">,</span> <span style=\"background-color: #BCBCFF\">price</span> <span style=\"background-color: #FFECEC\">and</span> <span style=\"background-color: #A8A8FF\">display</span> <span style=\"background-color: #B0B0FF\">stock</span> <span style=\"background-color: #C8C8FF\">in</span> <span style=\"background-color: #D0D0FF\">accordance</span> <span style=\"background-color: #EAEAFF\">with</span> <span style=\"background-color: #A6A6FF\">age</span> <span style=\"background-color: #FF9696\">uk</span></p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(len(data))\n",
    "sample = data.loc[i]\n",
    "print(\"Index:\", i)\n",
    "\n",
    "# predict salary on sample\n",
    "batch = generate_batch(sample.to_frame('original').transpose(), max_len=max_len)\n",
    "title_ix = torch.tensor(batch[\"Title\"], dtype=torch.int64).cuda()\n",
    "desc_ix = torch.tensor(batch[\"FullDescription\"], dtype=torch.int64).cuda()\n",
    "cat_features = torch.tensor(batch[\"Categorical\"], dtype=torch.float32).cuda()\n",
    "with torch.no_grad():\n",
    "    print(\"Salary (gbp):\", float(model(title_ix, desc_ix, cat_features)))\n",
    "\n",
    "tokens_and_weights = explain(model, sample, \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, sample, \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Actually make it work\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__.\n",
    "\n",
    "Try __at least 3 options__ from the list below for a passing grade. If you're into \n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm1d`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to do max pooling:\n",
    "* Max over time - our `GlobalMaxPooling`\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a small neural network\n",
    "\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$\n",
    "\n",
    "#### C) Fun with embeddings\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained word2vec from [here](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) or [here](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/).\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "  * Please bear in mind that while convolution uses [batch, units, time] dim order, \n",
    "    recurrent units are built for [batch, time, unit]. You may need to `torch.transpose`.\n",
    "\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [keras](https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L461) for inspiration.\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.state_dict`\n",
    "  * Plotting learning curves is usually a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I boosted training by moving it to GPU. \n",
    "\n",
    "Then, implemented option **E**. A snapshot of *model.state_dict* was made whenever best val MAE is beaten. Early stopping was done if no snap during *patience=5* epochs. Loss and MAE was plotted on each epoch using *livelossplot*.\n",
    "\n",
    "Next option - **C**. I used the pre-trained embeddings from the second link. The words not found in it (including Capital first letter and CAPS) where initialized randomly with a similar distribution (approx. std=0.13, mean=0). Also, the same embedding matrix was used in title and desc vectorizer. I found that freezing embedding layer weights for several epochs did not work better than training it from the beginning. So, the weights were not frozen in later experiments. For fair comparison, embeddings of 300 were used when training from scratch as well.\n",
    "\n",
    "**Findings:** using pre-trained embeddings improves the result. Freezeing them until loss saturates, then unfreezing, and training further improves relusts even more.\n",
    "\n",
    "After that, I experimented with ideas from **A** .\n",
    "\n",
    "**Findings:** dropout layer in various places with different probability parameter values did not improve the result in my experiments. Same goes for a BN layer after Conv in text encoders. Same goes for a BN layer right after concatenation or further dense layers  - it actually makes MAE unstable (oscillations at the end). BN in categorical encoder helps a lot though.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "<table>\n",
    " <tr>\n",
    "  <th></th>\n",
    "  <th>Random emb</th>\n",
    "  <th>Google's emb</th>\n",
    "  <th>Google's emb (freeze/unfreeze)</th>\n",
    "  <th>Google's emb (categorical BN)</th>\n",
    "  <th>Google's emb (3 parallel conv layers)</th>\n",
    "  <th>Google's emb (5 parallel conv layers)</th>\n",
    "  <th>All together</th>\n",
    " </tr>\n",
    " <tr>\n",
    "  <td>Best val MAE</td>\n",
    "  <td>2615.289</td>\n",
    "  <td>2499.929</td>\n",
    "  <td>2243.556</td>\n",
    "  <td>2386.140</td>\n",
    "  <td>2346.813</td>\n",
    "  <td>2260.447</td>\n",
    "  <td>2234.278</td>\n",
    " </tr>\n",
    "</table>\n",
    "\n",
    "So, the final MAE is 2234.3 which is better than in any of the \"one-trick\" experiments. However, I expected a bigger improvement compared to, for example, the \"freeze/unfreeze\" experiment. This difference may be just random. A possible explanation (this is a guess) is that the improvements used in the final version (BN, emb, freeze/unfreeze, 5 parallel conv, more layers) interact with each other.\n",
    "\n",
    "In some attempts of the \"explaining predictions\" task, I found that words like *manager, senior, sales, London, commercial* increases the salary estimation, why *assistant, inventory, php, marketing etc.* did the opposite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
